{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fsS9n8RqdjEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Query Using Langchain"
      ],
      "metadata": {
        "id": "gBtk_tC8zmC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rdAYZepFhJM",
        "outputId": "fc4d2a95-412b-4b3c-fe1c-70b2c43c6a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/983.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/983.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m727.0/983.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n",
            "  Downloading langchain_core-0.2.18-py3-none-any.whl (366 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.7 langchain-core-0.2.18 langchain-text-splitters-0.2.2 langsmith-0.1.85 orjson-3.10.6\n",
            "Collecting openai\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.13\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx9zmFh_LXSp",
        "outputId": "46105549-4632-4a8e-a8b1-b498a2f2f6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.7)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.18)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.7 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "v8fCmC-6Q3pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xP1-3VjZdlf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your own openai api\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "_aQ7ps_dRJOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('/content/pdf/Budget.pdf')"
      ],
      "metadata": {
        "id": "_FA1ZERdRLAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ],
      "metadata": {
        "id": "q9AeO9cDRqMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "yGlxUMl-Rsmy",
        "outputId": "3e9c3fb3-760a-4153-e508-3750bf939813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n \\nSEMESTER -3  \\n \\n \\n \\n \\n \\n \\n \\n1 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA201  DATA SCIENCE & \\nMACHINE LEARNING  CORE  3 1 0 4 \\nPreamble: This is an introductory course on data science and basic concepts behind various    \\nmachine learning techniques. Machine learning is the study of adaptive computational systems \\nthat improve their performance with experience. At the end of the course the students should \\nbe able to design and implement machine learning solutions to classification, regression, and \\nclustering problems and to evaluate an d interpret the results of the algorithms.  \\nPrerequisite:  Probability and Statistics, Linear Algebra, Programming in Python/R.  \\nCourse Outcomes: After the completion of the course the student will be able to:  \\nCO No.  Course Outcome (CO)  Bloom\\'s Category \\nLevel  \\nCO 1  Discuss  the fundamental concepts of data science and data \\nvisualization techniques . Level 2:  \\nUnderstand  \\nCO 2  Explain  the basics of machine learning and use lazy learning \\nand probabilistic learning algorithms to solve data science \\nproblems . Level 3:  \\nApply  \\nCO 3  Describe decision tree s, classification rules  & regression \\nmethods and how these algorithms can be applied to solve data \\nscience  problems . Level 3:  \\nApply  \\nCO 4  Solve data science problems  using  neural networks and \\nsupport vector machines . Level 3:  \\nApply  \\nCO 5  Discuss clustering using k -means algorithm  and evaluate & \\nimprove the performance of machine learning classification \\nmodels.  Level 3:  \\nApply  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  1 1 - - -  -  3  -  - -  - - \\nCO 2  3  3 3 2 - - 3 - - - - - \\nCO 3  3 3 3 2 - - 3 - - - - - \\nCO 4  3  3  3  2 - -  3 - - - - - \\nCO 5  3 3  3  2 - -  3 - - - - - \\n  3/2/1: High/Medium/Low  \\n2 \\n  \\nAssessment Pattern  \\nBloom’s Category  Continuous Assessment \\nTests  End Semester \\nExamination  \\n1 2 \\nRemember (K1)  15 10 10 \\nUnderstand (K2)  25 20 30 \\nApply (K3)  10 20 20 \\nAnalyse (K4)     \\nEvaluate (K5)     \\nCreate (K6)        \\nMark Distribution  \\nTotal Marks  CIE ESE  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\nEnd Semester Examination Pattern:  There will be two parts: Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module, of which the student should answer any \\none. Each question c an have a maximum of 2 subdivisions and carry 6 marks.  \\n Sample Course Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n1. What is data science and why do we need data science?  \\n2. Explain the data science classification and illustrate data science tasks.  \\n3. Describe  the various methods to understand data.  \\n4. Explain the typical methods to visualize data.  \\n \\n3 \\n  \\nCourse Outcome 2 (CO2)   \\n1. Explain the differences between supervised and unsupervised machine learning \\nalgorithms.  \\n2. Describe the key concepts  that define nearest neighbour classifiers, and why they are \\nconsidered \"lazy\" learners.  \\n3. Explain how to apply k-NN classifier in a data science problem.  \\n4. State B ayes\\' theorem in statistics. Outline  the Naive Bayes algorithm to build \\nclassification models.  \\n5. Use Naive Bayes algorithm to determine whether a red domestic SUV car is a \\nstolen car  or not using the following data:  \\nExample  Colour  Type  Origin  Stolen?  \\n1 red sports  domestic  yes \\n2 red sports  domestic  no \\n3 red sports  domestic  yes \\n4 yellow  sports  domestic  no \\n5 yellow  sports  imported  yes \\n6 yellow  SUV  imported  no \\n7 yellow  SUV  imported  yes \\n8 yellow  SUV  domestic  no \\n9 red SUV  imported  no \\n10 red sports  imported  yes \\nCourse Outcome 3  (CO3):  \\n1. Classify data science  tasks  using decision trees and classification rule learners . \\n2. Discuss  the various feature selection measures.  \\n3. How to simplify a decision tree by pruning.  \\n4. Describe h ow to construct classification rules from decision trees.  \\n5. Explain the concepts of regression and correlation.  \\n6. How to estimate a linear regression model.  \\n7. Consider the following set of training examples:  \\nInstance  Classification      a1     a2 \\n1 + T T \\n2 + T T \\n4 \\n 3 - T F \\n4 + F F \\n5 - F T \\n6 - F T \\na) Find the entropy of this collection of training examples with respect to the target \\nfunction “classification”?  \\nb) Calculate  the information gain of a2 relative to these training examples?  \\nCourse Outcome 4  (CO4):  \\n1. Explain how artificial neural networks mimic human  brain  to model arbitrary \\nfunctions and how these can be applied to real -world problems.  \\n2. Describe different activation functions and  network topology.  \\n3. Discuss  basic idea  behind the backpropagation algorithm.  \\n4. Explain how a support vector machine can be used for classification of linearly \\nseparable data.  \\n5. How to compute the distance of a point from a hyperplane.  \\n6. How the kernel trick is used to construct classifiers in nonlinearly separated data.  \\nCourse Outcome 5 (CO5):  \\n1. Explain how the  clustering tasks differ from the classification tasks.  \\n2. How clustering defines a group, and how such groups are identified by k-means \\nclustering algorithm.  \\n3. Find the three clusters after one epoch for the following eight examples using the k-\\nmeans algorithm and Euclidean distance: A1 = (2,10), A2 = (2,5), A3 = (8,4), A4 = \\n(5,8), A5 = (7,5), A6 = (6,4), A7 = (1,2), A8 = (4,9). Suppose that the initial seeds \\n(centres of each cluster) are A1, A4 and A7.  \\n4. Explain the various matrices used to measure the performance of classification \\nalgorithms  \\n5. Explain the concepts  of bagging and boosting.  \\n6. Suppose 10000 patients get tested for flu; out of them, 9000 are actually healthy and \\n1000 are actually sick. For the sick people, a test was positive for 620 and negative \\nfor 380. For the healthy people, the same test was positive for 180 and negative for \\n8820. Construct a confusion matrix for the data and compute the precision and recall \\nfor the data.  \\n  \\n5 \\n Model Question Paper   \\nCourse Code: 20MCA201  \\n \\n Course Name: DATA SCIENCE AND MACHINE LEARNING  \\n                                                   \\nMax. Marks :60        Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1. What is data science?  \\n2. Explain the different types of data.  \\n3. Differentiate between supervised and unsupervised learning algorithms.  \\n4. Explain how to choose t he value of k in k-NN algorithm.  \\n5. Explain entropy and information gain.  \\n6. Explain the Ordinary Least Square  method in regression.  \\n7. Define activation function. Give two examples.  \\n8. What is  maximum margin hyperplane.  \\n9. Define precision, recall and F-measure.  \\n10. Explain bootstrap sampling  \\nPart B  \\nAnswer one full question from each module, each carries 6 marks.  \\n11. Explain the various methods for visualising multivariate data.   (6 marks)  \\nOR \\n12. Explain the various processes for preparing a dataset to perform a data science \\ntask.         (6 marks)  \\n13. Based on a survey conducted in an institution, students are classified based on the \\ntwo attributes of academic excellence and other activities. Given the following data, \\nidentify the classificat ion of a student with X = 5 and Y = 7 using k-NN algorithm \\n(choose k as 3).  \\nX (Academic Excellence)  Y (Other Activities)  Z (Classification)  \\n8 6 Outstanding  \\n5 6 Good  \\n7 3 Good  \\n6 9 Outstanding  \\n(6 marks)   \\nOR \\n6 \\n 14. Given the following data on a certain set of patients seen by a doctor . Can the doctor \\nconclude that a person having chills, fever, mild headache and without running nose \\nhas flu? (Use Naive Bayes classification).  \\nChills  Running nose  Headache  Fever  Has flu  \\nY N mild Y N \\nY Y no N Y \\nY N strong  Y Y \\nN Y mild Y Y \\nN N no N N \\nN Y strong  Y Y \\nN Y strong  N N \\nY Y mild Y Y \\n(6 marks)  \\n15. Obtain a linear regression for the data given in the table below assuming that y is \\nthe independent variable.  \\nx 55 60 65 70 80 \\ny 52 54 56 58 62 \\n(6 marks)  \\nOR \\n16. Given the following data, draw a decision tree to predict whether a person cheat s. \\nGive the corresponding set of classification rules also.  \\nSl. No.  Refund  Marital status  Income  Cheat s? \\n1 Yes Single  High  No \\n2 No Married  High  No \\n3 No Single  Low No \\n4 Yes Married  High  No \\n5 No Divorced  High  Yes \\n6 No Married  Low No \\n7 \\n 7 Yes Divorced  High  No \\n8 No Single  High  Yes \\n9 No Married  Low No \\n10 No Single  High  Yes \\n(6 marks)  \\n17. Define an artificial neuron. What are the characteristics of an artificial neural \\nnetwork (ANN)?  \\n(6 marks)  \\nOR \\n18. a) Define linearly separable dataset. Give an example each of a dataset that is \\n linearly separable and of a dataset that is not linearly  separable.  \\n(3 marks)  \\nb) Define kernel function. Explain the kernel trick to construct a classifier for a \\n dataset that is not linearly separable.  \\n(3 marks)  \\n19. Suppose 10000 patients get tested for flu; out of them, 9000 are actually healthy \\nand 1000 are actually sick. For the sick people, a test was positive for 620 and \\nnegative for 380. For the healthy people, the same test was positive for 180 and \\nnegative for 8820. Construct a confusion matrix for the data and compute the \\nprecision and recall for the d ata.          (6 marks)  \\nOR \\n20. Assume the following: A database contains 80 records on a particular topic of which \\n55 are relevant to a certain investigation. A search was conducted on that topic and \\n50 records were retrieved. Of the 50 records retrieved, 40 were relevant. Construct \\nthe confusion matrix for the search and calculate the precision and recall scores for \\nthe search.             (6 marks)  \\n \\n \\n \\n \\n \\n \\n \\n \\n8 \\n Syllabus  \\nModule 1 (9 Hours)  \\nIntroduction to data science, Data science classification , Data science process  - Prior \\nknowledge , Data preparation , Modelling , Application , Data exploration  - Data sets , \\nDescriptive statistics for univariate and multivariate data  \\nData visualisation  – Histogram , Quartile plot , Distribution chart , Scatter plot , Bubble c hart, \\nDensity chart  \\nModule 2  (9 Hours)  \\nIntroduction to machine learning:  How machines learn - Data storage , Abstraction , \\nGeneralisation , Evaluation , Machine learning in practice - Types of machine learning \\nalgorithms.  \\nLazy learning:  Classification using K-Nearest Neighbour algorithm - Measuring similarity \\nwith distance , Choice of k , Preparing data for use with k -NN. \\nProbabilistic learning:  Understanding Naive Bayes - Conditional probability and Bayes \\ntheorem , Naive Bayes algorithm for classification , The Laplace estimator, Using numeric \\nfeatures with Naive Bayes . \\nModule 3  (9 Hours)  \\nDecision tree learning:  Concept of decision tree , Divide and conquer approach , C5.0 \\nDecision tree algorithm , Choosing the best split , Pruning the decision tree . \\nClassification rules learning:  Concept of classification rules , Separate and conquer \\napproach , The 1R algorithm , Rules from decision trees . \\nRegr ession methods:  Concept of regression , Simple linear regression , Ordinary least \\nsquares estimation , Correlations , Multiple linear regression . \\nModule 4  (9 Hours)  \\nNeural network learning:  Artificial neurons , Activation functions , Network topology , \\nTraining neural networks with backpropagation . \\nSupport vector machines:  Hyperplanes , Classification using hyperplanes , Maximum \\nmargin hyperplanes in linearly separable data , Using kernels for non -linear spaces .  \\nModule 5  (9 Hours)  \\n \\nClustering:  The k -means clustering algorithm , Using distance to assign and update clusters , \\nChoosing number of clusters.  \\nEvaluating model performance:  Confusion matrices , Precision and recall , Sensitivity and \\nspecificity , Precision and recall , F-measure , ROC curves , Cross validation  - K-fold cross \\nvalidation , Bootstrap sampling . \\nImproving model performance  - Bagging , Boosting , Random forests . \\n  \\n9 \\n Text Books  \\n1. Vijay Kotu, Bala  Deshpande, Data Science Concepts and Practice, Morgan Kaufmann \\nPublishers 2018 (Module 1)  \\n2. Brett Lantz, Machine Learning with R, Second edition, PackT publishing 2015 \\n(Modules 2 to 5)  \\nReference Books  \\n1. Michael Steinbach, Pang -Ning Tan, and Vipin Kumar, Intro duction to Data Mining, \\nPearson 2016.  \\n2. Jiawei Han, Micheline Kamber and Jian Pei, Data mining Concepts and techniques, \\nMorgan Kaufmann Publishers 2012  \\n3. Peter Harrington, Machine Learning in action, Dreamtech publishers 2012  \\n4. Dr M Gopal, Applied Machine learning, McGraw Hill Education Private Limited  \\n5. E. Alpayidin, Introduction to Machine Learning, Prentice Hall of India (2005)  \\n6. T. Hastie, RT Ibrashiran and J. Friedman, The Elements of Statistical Learning,  \\nSpringer 2001  \\n7. Data Science from Scratch: First Pri nciples with Python, Joel Grus, O’Reilly, First \\nedition, 2015  \\n8. Introducing Data Science, Davy Cielen, Arno D. B. Meysman, Mohamed Ali, Manning  \\nPublications Co., 1st edition, 2016  \\nWeb Resources:  \\n1. https://www.coursera.org/learn/machine -learning  \\n2. https://www.coursera.org/learn/data -scientists -tools  \\nCourse Contents and Lecture Schedule  \\n \\n Topic  No. of \\nLectures  \\n1 Module 1  9 hrs  \\n1.1 Introduction to data science  - What is data science?  Why data science?  2 hrs  \\n1.2 Data science classification  1 hr \\n1.3 Data science process  - Prior knowledge , Data preparation , Modelling , \\nApplication  2 hrs  \\n1.4 Data exploration - Data sets , Descriptive statistics for univariate and \\nmultivariate data  2 hrs  \\n1.5 Data visualization  – Histogram , Quartile plot , Distribution chart , Scatter \\nplot, Bubble chart , Density chart  2 hrs  \\n2 Module 2  9 hrs  \\n10 \\n 2.1 How machines learn – Data storage – Abstraction – Generalisation – \\nEvaluation  1hr \\n2.2 Machine learning in practice – Types of machine learning algorithms.  1hr \\n2.3 Classification: Lazy learning - K-Nearest Neighbour algorithm  2 hrs  \\n2.4 Measure of similarity , Choice of k  1 hr \\n2.5 Preparing data for use with k -NN 1 hr \\n2.6 Probabilistic Learning: Conditional probability and Bayes theorem.  1 hr \\n2.7 Naive Bayes algorithm  2 hrs  \\n3 Module 3  9 hrs  \\n3.1 Concept of decision tree , Divide and conquer approach  1 hr \\n3.2 C5.0 Decision tree algorithm  1 hr \\n3.3 Choosing the best split , Pruning the decision tree  2 hrs  \\n3.4 Classification rules learning: Concept of classification rules , Separate \\nand conquer approach  1 hr \\n3.5 The 1R algorithm , Rules from decision trees  1 hr \\n3.6 Regression methods: Concept of regression , Correlations  1 hr \\n3.7 Simple linear regression , Ordinary least squares estimation  1 hr \\n3.8 Multiple linear regression  1 hr \\n4 Module 4  9 hrs  \\n4.1 Understanding neural networks  - Artificial neurons  1 hr \\n4.2 Activation functions, Network topology  2 hrs  \\n4.3 Training neural networks with back propagation  1hr \\n4.4 Understanding Support Vector Machines , Classification with \\nhyperplane  1 hr \\n4.5 Linearly separable data, Nonlinearly separable data  1 hr \\n4.6 Methods to find maximum margin hyperplanes in linearly separable \\ndata 1 hr \\n4.7 Using kernels for non -linear spaces  2 hrs  \\n5 Module 5  9 hrs  \\n5.1 Understanding Clustering - The k -means clustering algorithm  1 hr \\n11 \\n 5.2 Using distance to assign and update clusters, Choosing the appropriate \\nnumber of clusters  1 hr \\n5.3 Evaluating model performance: Confusion matrices , Precision and \\nrecall , Sensitivity and specificity , Precision and recall , F-measure , ROC \\ncurves.  2 hrs  \\n5.4 Cross validation: K -fold cross validation , Bootstrap sampling  2 hrs  \\n5.5 Improving model performance: Bagging , Boosting  2 hrs  \\n5.6 Random forests  1 hr \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n12 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA203  DESIGN & ANALYSIS OF \\nALGORITHMS  CORE  3 1 0 4 \\n \\nPreamble:  The syllabus is prepared with a view to provide a strong foundation to students in \\ndesign and analysis of computer algorithms and to introduce them the advanced topics such as \\nNetwork Flows, Approximation algorithms and Randomised algorithms.  \\nPrerequisite:  Knowledge in Data Structures  \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Discuss the basic concepts in computer algorithms and their \\nanalysis & design using Divide and Conquer.  Level 2: \\nUnderstand  \\nCO 2  Explain the concepts of Greedy Strategy and Dynamic \\nProgramming to use it in solving real world problems.  Level 3:  \\nApply  \\nCO 3  Explain the Branch & Bound technique, Backtracking \\ntechnique and Lower bounds.  Level 2: \\nUnderstand  \\nCO 4  Describe the fundamental concepts of Computational \\nComplexity and Network Flows.  Level 2: \\nUnderstand  \\nCO 5  Discuss the concepts of Approximation and Randomised \\nAlgorithms.  Level 2:  \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n PO 1  PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 3 1 2   2      \\nCO 2  3 3 1 2   2      \\nCO 3  3 3 1 2   2      \\nCO 4  3 3 1 2   2      \\nCO 5  3 3 1 2   2      \\n  3/2/1: High/Medium/Low  \\n \\n \\n \\n \\n13 \\n Assessment Pattern  \\nBloom’s  \\nCategory Levels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2  \\nLevel 1: Remember  20 20 20 \\nLevel 2: Understand  20 30 30 \\nLevel 3: Apply  10  10 \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination (ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance       : 8 marks  \\nContinuous Assessment Test (2 numbers)   : 20 marks  \\nAssignment/Quiz/Course project    : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A  and Part B . Part A \\ncontain 10 questions with 2 questions from each module, having 3 marks for each questi on. \\nStudents should answer all questions. Part B contains 2 questions from each module of which \\nstudent should answer any one . Each question can have a maximum 2 subdivisions and carry \\n6 marks.  \\n Sample Course Level Assessment Questions  \\n Course Outcome 1 ( CO 1):  \\n1. Define “Time Complexity” of an algorithm?  \\n2. What is the need for analysing an algorithm?  \\n3. Define Big Oh Notation.  \\n4. Define the terms Best Case, Worst Case and Average case complexities.  \\n5. Explain the Merge Sort algorithm with an example.  \\nCourse Outcome 2 (CO 2):  \\n1. Explain the Greedy Control abstraction.  \\n2. Write the Prim’s algorithm and illustrate with an example.  \\n3. State and illustrate the Principle of Optimal Substructure.  \\n4. Explain a solution to the Travelling Salesman problem using Dynamic Programming . \\n14 \\n Course Outcome 3 (CO 3):  \\n1. Explain the N -Queen’s problem and its solution using Backtracking.  \\n2. Explain the 8 -puzzle problem and illustrate how it can be solved using Branch and \\nBound.  \\n3. Bring out the notion of Decision Trees.  \\n4. What is the lower bound of the time complexity of Comparison based sorting \\nalgorithms?  \\nCourse Outcome 4 (CO 4):  \\n1. Define class P and NP.  \\n2. What is Polynomial Time Reduction?  \\n3. Show that the Clique problem is NP -Complete.  \\n4. Define the Terms - Flow Network and Network Flow.  \\n5. Explain the Ford -Fulke rson Algorithm.  \\nCourse Outcome 5 (CO 5):  \\n1. What is an Approximation algorithm?  \\n2. Describe the 2 -approximation algorithm for Vertex Cover problem.  \\n3. What is a Randomised algorithm?  \\n4. Explain the Schwartz -Zippel Lemma. How this Lemma can be used to test the identi ty \\nof two polynomials.  \\nModel Question Paper  \\nCourse Code: 20MCA203  \\n \\nCourse Name: Design and Analysis of Algorithms  \\n \\n \\nMax.  Marks :60            Duration: 3 Hrs  \\nPart A  \\nAnswer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1. Define Big Oh notation.  \\n2. Write the control abstraction for a typical Divide and Conquer algorithm.  \\n3. Explain a Greedy strategy which can give the optimal solution for the Knapsack problem.  \\n4. Write a dynamic programming algorithm to compute the factorial of a number.  \\n5. How does Backtracking differ from Branch and Bound?  \\n6. Using a decision tree, show that any search algorithm which searches a given key within \\nan array of n elements must perform at least O(ln n) comparisons in the worst case.  \\n7. What do you mean by the term Polynom ial time reduction?  \\n8. Define the term Network Flow and illustrate with an example.  \\n15 \\n 9. What do you mean by approximation ratio of an Approximation algorithm?  \\n10. What is meant by a Randomised Algorithm?  \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n \\n11 Write the Linear Search Algorithm and analyse the best, worst and average case \\ncomplexities of the algorithm.  6 \\n OR  \\n12 Explain the Merge Sort algorithm and give its worst -case analysis.  6 \\n     \\n13 Write Kruskal\\'s algorithm to compute the minimum cost spanning tree.  6 \\n  OR  \\n14 Explain the dynamic programming algorithm for the Travelling Salesman problem.  6 \\n15 Write the Backtracking algorithm for N -Queen Problem.  6 \\n  OR  \\n16 Explain the 8-puzzle problem and its solution using branch and bound technique.  6 \\n17 Show that the Clique problem is NP -Complete.  6 \\n  OR  \\n18 Describe the Ford Fulkerson’s procedure to compute the Max -Flow within a given \\nFlow Network.  6 \\n19 Explain the 2 -approximation algorithm for Vertex Cover and justify its \\napproximation ratio.  6 \\n  OR  \\n20 Describe Randomised Quick sort.  6 \\n \\n \\n \\n  \\n16 \\n Syllabus  \\nModule 1: (8 Hours)  \\nReview of Algorithm Analysis : Time and Space Complexity, Asymptotic Notations, \\nRecurrence Equations, Solving Recurrence Equations - Substitution method and Iteration \\nmethod.  \\nDivide and Conquer :  Control Abstraction, Merge Sort,  Quick Sort, Matrix Multiplication.  \\nModule 2: ( 9 Hours)  \\nGreedy Strategy : Control Abstraction, Knapsack Problem, Minimal Spanning Tree \\nAlgorithms - Prim’s and Kruskal’s Algorithm, Job Scheduling with deadlines  \\nDynamic Programming : Control Abstraction, Principle of Optimal Substructure, All Pairs \\nshortest path problem, Travelling Salesman Problem , Bellman -Ford  Algorithm  \\nModule 3:(7 Hours)  \\nBacktracking:  Control Abstraction, N -Queens problem, Sum of Subsets Problem  \\nBranch and Bound : Control Abstraction, 8 - Puzzle problem  \\nLower Bounds:  The Decision Tree method, Lower Bounds for Comparison based Sort and \\nSearching (Analysis not required)  \\nModule 4: (1 1 Hours)  \\nComplexity Theory :  Class P and NP, Polynomial time reductions, Class NP Hard and NP -\\nComplete, Example Problems - Vertex Cover problem, Clique Problem.  \\nNetwork Flows :  Flow Networks and Network Flow, Max - Flow Min Cut Theorem, Ford \\nFulkerson metho d, Bipartite matching ( Analysis not required)  \\nModule 5: (10 Hours)  \\nIntroduction to Approximation Algorithms : Approximation Ratio, 2 -approximation \\nalgorithm for Vertex Cover problem, Vertex Cover Approximation using Linear Programming \\nand LP Rounding Algorithm.  \\nIntroduction to Randomised Algorithms : Review of Basic Probability, Schwartz -Zippel \\nLemma and Polynomial Identity Testing, Randomized Quick Sort ( Proof of Expected Worst \\nCase Analysis not required)  \\nText Books  \\n1. Thomas H. Cormen, et al ., “Introduction to Algorithms”, Prentice Hall, 3rd Edition \\n(2010)  \\n2. Ellis Horowitz, Sartaj Sahni, Sanguthevar Rajasekaran, “Fundamentals of Computer \\nAlgorithms”, Orient Longman, Universities Press, 2nd Edition (2008)  \\n17 \\n Reference Books  \\n1. Richard Neapolitan , Kumarss Naimipour, “Foundations of Algorithms ”, Jones and \\nBartlett Publishers, Inc, 4th Edition (2011).  \\n2. Sara Baase, Allen Van Gelder, “Computer Algorithms: Introduction to Design and \\nAnalysis”, Pearson India, 3rd Edition (2002).  \\n3. A. Levitin, “I ntroduction to the Design & Analysis of Algorithms”, Pearson Education, \\n3rd Edition (2008).  \\nCourse Contents and Lecture Schedule  \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Review of Algorithm Analysis and Divide & Conquer  8 Hours  \\n1.1 Time and Space Complexity  1 \\n1.2 Asymptotic Notations  1 \\n1.3 Recurrence Equations , Solving Recurrence Equations - Substitution \\nmethod  1 \\n1.4 Iteration method  1 \\n1.5 Divide and Conquer:  Control Abstraction, Merge Sort , Merge Sort \\nAnalysis  2 \\n1.6 Quick Sort , Quicksort analysis  1 \\n1.7 Matrix Multiplication  1 \\n2 Greedy Strategy and Dynamic Programming  9 Hours  \\n2.1 Greedy Strategy: Control Abstraction, Knapsack Problem  1 \\n2.2 Minimum Cost Spanning Tree  1 \\n2.3 Prim’s algorithm  1 \\n2.4 Kruskal’s algorithm  1 \\n2.5 Job Scheduling with deadlines  1 \\n2.6 Dynamic Programming : Control Abstraction, Principle of Optimal \\nsubstructure  1 \\n2.7 All Pairs shortest path problem  1 \\n2.8 Travelling Salesman Problem  1 \\n2.9 Bellman -Ford  Algorithm  1 \\n18 \\n 3 Backtracking, Branch & Bound, Lower Bounds  7 Hours  \\n3.1 Backtracking:  Control Abstraction  N- Queens problem  1 \\n3.2 Sum of subsets problem  1 \\n3.3 Branch and Bound:  Control Abstraction  8- Puzzle problem  1 \\n3.4 Lower Bounds : The Decision Tree method  2 \\n3.5 Lower Bounds for Comparison based Sorting  1 \\n3.6 Lower bounds for searching  1 \\n4 Computational complexity, Network  Flows  11 Hours  \\n4.1 Class P, NP  1 \\n4.2 Polynomial Time Reductions  1 \\n4.3 Class NP -Hard and NP -Complete  2 \\n4.4 Vertex Cover Problem  1 \\n4.5 Clique problem  1 \\n4.6 Flow Networks and Network Flows  2 \\n4.7 Max Flow Min Cut Theorem  1 \\n4.8 Ford  Fulkerson’s method  1 \\n4.9 Bipartite matching  1 \\n5 Approximation & Randomised Algorithms  10 Hours  \\n5.1 Approximation algorithms - introduction, Approximation Ratio  1 \\n5.2 2- approximation algorithm for Vertex Cover problem  1 \\n5.3 Vertex Cover Approximation using Linear Programming and LP \\nRounding Algorithm  2 \\n5.4 Randomized Algorithms : introduction, Review of Basic Probability  1 \\n5.5 Review of Basic probability  2 \\n5.6 Schwartz -Zippel Lemma and Polynomial Identity Testing  2 \\n5.7 Randomized Quick Sort  1 \\n \\n \\n \\n19 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA261  OPERATIONS RESEARCH  ELECTIVE  3 1 0 4 \\n \\nPreamble:  This course introduces the concepts of linear programming problems. The topics \\ntreated in this course have applications in real life problems.   \\nPrerequisite: Nil \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Solve different types of L inear Programming Problems .  Level 3:  \\nApply  \\nCO 2  Apply the concept of linear programming problems  in real \\nlife. Level 3:  \\nApply  \\nCO 3  Solve different decision -making  problems using \\noptimization techniques.  Level 3:  \\nApply  \\nCO 4  Use PERT and CPM to  analyse project network \\nmanagement.  Level 3:  \\nApply  \\nCO 5  Identify suitable  queuing model and solve  queuing \\nproblems . Level 3:  \\nApply  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 3 1 -    -  - 2  -  - -  - -  \\nCO 2  3 3 3 -  -  -  2 - - - - -  \\nCO 3  3 3 3 -  -  - 2 - - - - -  \\nCO 4  3 3 1 1  -  - 2 2 - - - -  \\nCO 5  3 3 3 -  -  - 2 - - - - -  \\n  3/2/1: High/Medium/Low  \\n \\n \\n \\n \\n \\n \\n20 \\n Assessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End Semester \\nExamination  \\n1 2 \\nLevel 1: Remember  10 10 10 \\nLevel 2: Understand  20 20 20 \\nLevel 3: Apply  20 20 30 \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\n  \\nMark Distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance       : 8 marks  \\nContinuous Assessment Test (2 numbers)   : 20 marks  \\nAssignment/Quiz/Course project    : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontain 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can have maximum 2 sub -divisions and carry 6 marks  \\n \\nSample Course L evel Assessment Questions  \\nCourse Outcome 1 (CO 1):  \\n1. Define slack variable, surplus variable and optimal basic feasible solution.  \\n2. Obtain all basic feasible solution of the set of equations:  \\na) 2𝑥1+3𝑥2+4𝑥3+𝑥4=2 \\nb) 𝑥1+𝑥2+7𝑥3+𝑥4=4 \\n3. Solve by Big M method  \\nMaximise  𝑍=6𝑥1−3𝑥2+2𝑥3 \\nSubject to  2𝑥1+𝑥2+𝑥3≤16 \\n3𝑥1+2𝑥2+𝑥3≤18 \\n21 \\n 𝑥1−2𝑥2≥8 \\n𝑥1, 𝑥2, 𝑥3≥0 \\nCourse Outcome 2 (CO 2):  \\n1. Construct the dual of  \\nMaximise  𝑍=3𝑥1+17𝑥2+9𝑥3 \\nSubject to  𝑥1−𝑥2+𝑥3≥3 \\n−3𝑥1+2𝑥2≤1 \\n𝑥1, 𝑥2, 𝑥3≥0 \\n2. Prove that the dual of the dual is the primal  \\n3. Solve using the principle of duality  \\nMinimise  𝑍=3𝑥1+5𝑥2 \\nSubject to  2𝑥1+8𝑥2≥40 \\n3𝑥1+4𝑥2≥50 \\n𝑥1, 𝑥2≥0 \\nCourse Outcome 3 (CO 3):  \\n1. Explain North West Corner method  \\n2. Solve the following transportation problem  \\n 1 2 3 Supply  \\n1 2 7 4 5 \\n2 3 3 1 8 \\n3 5 4 7 7 \\n4 1 6 2 14 \\nDemand  7 9 18 34 \\n \\n3. Solve the assignment problem  \\n I II III IV \\nA 16 10 14 11 \\nB 14 11 15 15 \\nC 15 15 13 12 \\nD 13 12 14 15 \\n \\n22 \\n Course Outcome 4 (CO 4):  \\n \\n1. Explain critical path analysis.  \\n2. A project consists of series of tasks labelled A, B, …, H, I with the following \\nrelationships (W < X, Y means X and Y cannot start until W is completed; X, Y < W \\nmeans W cannot start until both X and Y are completed). With this notation cons truct \\nthe network diagram having the following constraints:  \\nA < D, E;  B, D < F;  C < G;  B, G< H;  F, G < I.  \\nFind also the minimum time of completion of the project, when the time (in days) of \\ncompletion of each task is as follows:  \\nTask :  A B C D E F G H I \\nTime :  23 8 20 16 24 18 19 4 10 \\n \\n3. A project consists of eight activities with the following relevant information.  \\nActivity  Immediate \\npredecessor  Estimated duration (days)  \\nOptimistic  Most likely  Pessimistic  \\nA -- 1 1 7 \\nB -- 1 4 7 \\nC -- 2 2 8 \\nD A 1 1 1 \\nE B 2 5 14 \\nF C 2 5 8 \\nG D, E 3 6 15 \\nH F, G 1 2 3 \\n \\n(i) Draw the PERT network and find out the expected project completion time.  \\n(ii) What duration will have 95% confidence for project completion?  \\n(iii) If the average duration for activity F increases to 14 days, what will be its \\neffects on the expected project completion time which will have 95% \\nconfidence?  \\n(For standard normal Z = 1.645, area under the standard normal curve from 0 to Z is \\n0.45)  \\nCourse Outcome 5 (CO 5):  \\n1. Explain Birth -death process.  \\n2. In a railway ma rshalling yard, goods trains arrive at a rate of 30 trains per day. \\nAssuming that the inter -arrival time follows an exponential distribution and the \\n23 \\n service time distribution is also exponential with an average 36 minutes. Calculate \\nthe following:  \\ni. The mean queue size (line length), and  \\nii. The probability that the queue size exceeds 10.  \\niii.       If the input of trains increases to an average 33 per day, what will be the change in \\n       (i) and (ii)?  \\n3. At a railway station, only one train is handled at a time. T he railway yard is sufficient \\nonly for two trains to wait while other is given signal to leave the station. Trains \\narrive at the station at an average rate of 6 per hour and the railway station can handle \\nthem on an average of 12 per hour. Assuming Poisson  arrivals and exponential service \\ndistribution, find the steady -state probabilities for the various number of trains in the \\nsystem. also find the average waiting time of a new train coming into the yard  \\n \\n \\nModel Question Paper  \\nCourse Code: 20MCA261  \\nCourse name: Operations Research  \\nMax. Marks: 60               Duration: 3hrs  \\nPart A  \\nAnswer all questions, each carries 3 marks (10 ×3 = 30)  \\n1. Write down the basic structure of a linear programming problem in the mathematical \\nform.  \\n2. Define slack and surplus variables in LPP.  \\n3. State the fundamental theorem of duality.  \\n4. Write the dual of the following  \\n 𝑀𝑎𝑥  𝑍=𝑥1−𝑥2+3𝑥3 \\n 𝑠𝑢𝑏𝑗𝑒𝑐𝑡  𝑡𝑜 𝑥1+𝑥2+𝑥3≤10 \\n           2𝑥1−𝑥3≤2 \\n          2𝑥1−2𝑥2+3𝑥3≤6 \\n           𝑥1,𝑥2,𝑥3≥0 \\n5. Obtain the IBFS using north west corner method  \\n D1 D2 D3 D4 Supply  \\nO1 2 4 3 6 20 \\nO2 7 3 8 2 10 \\nO3 2 2 9 11 15 \\nDemand  15 15 8 7  \\n24 \\n 6. Describe the Matrix Minima method.  \\n7. What is queue discipline?  \\n8. Explain single serve Poisson queuing model with infinite capacity.  \\n9. Activities P, Q and R instantly follow activity M, and their current starting times are \\n12, 19 and 10. So, what is the latest finishing time for activity M?  \\n10. What is the difference between PERT and CPM.  \\n \\nPart B  \\nAnswer all questions, each carries 6 marks (5 ×6 = 30)  \\n11. Solve the following problem by Simplex method  \\n  𝑀𝑎𝑥  𝑍=5𝑥1+3𝑥2 \\n 𝑠𝑢𝑏𝑗𝑒𝑐𝑡  𝑡𝑜 4𝑥1−𝑥2≤10 \\n          2𝑥1+2𝑥2≤50 \\n           𝑥1,𝑥2≥0 \\nor \\n12. Solve by Big -M method  \\n  𝑀𝑎𝑥  𝑍=6𝑥1−3𝑥2+2𝑥3 \\n 𝑠𝑢𝑏𝑗𝑒𝑐𝑡  𝑡𝑜 2𝑥1+𝑥2+𝑥3≤16 \\n           3𝑥1+2𝑥2+𝑥3≤18 \\n           𝑥2−2𝑥3≥8 \\n           𝑥1,𝑥2,𝑥3≥0 \\n13. Prove that the dual of a dual is the primal.  \\nor \\n14. Solve the following by using the dual principle  \\n  𝑀𝑎𝑥  𝑍=40𝑥1+35𝑥2 \\n 𝑠𝑢𝑏𝑗𝑒𝑐𝑡  𝑡𝑜 2𝑥1+3𝑥2≤60 \\n          4𝑥1+3𝑥2≤96 \\n           𝑥1,𝑥2≥0 \\n15. Solve the following Assignment problem  \\n I II  III IV \\nA 2 3 4 5 \\nB 4 5 6 7 \\nC 7 8 9 8 \\nD 3 5 8 9 \\n25 \\n  \\nor \\n16. Solve the following transportation problem  \\n D1 D2 D3 D4 Supply  \\nO1 5 2 4 3 22 \\nO2 4 5 1 6 15 \\nO3 4 6 7 5 8 \\nDemand  7 12 17 9  \\n17. Explain critical path analysis.  \\nor \\n18. A project consists of eight activities with the following relevant information.  \\nActivity  Immediate \\npredecessor  Estimated duration (days)  \\nOptimistic  Most likely  Pessimistic  \\nA -- 1 1 7 \\nB -- 1 4 7 \\nC -- 2 2 8 \\nD A 1 1 1 \\nE B 2 5 14 \\nF C 2 5 8 \\nG D, E 3 6 15 \\nH F, G 1 2 3 \\n \\n(iv) Draw the PERT network and find out the expected project completion time.  \\n(v) What duration will have 95% confidence for project completion?  \\n(vi) If the average duration for activity F increases to 14 days, what will be its effects \\non the expected project completion time which will have 95% confidence?  \\n(For standard normal Z = 1.645, area under the standard normal curve from 0 to Z is \\n0.45)  \\n19. Explain birth -death process.  \\nor \\n20. At a railway station, only one train is handled at a time. The railway yard is sufficient \\nonly for two trains to wait while other is given signal to leave the station. Trains arrive \\nat the station at an average rate of 6 per hour and the railway station can handle them \\non an average  of 12 per hour. Assuming Poisson arrivals and exponential service \\ndistribution, find the steady -state probabilities for the various number of trains in the \\nsystem. also find the average waiting time of a new train coming into the yard . \\n26 \\n Syllabus  \\nModule 1: (9 Hours ) \\nLinear programming problem - Slack and surplus variable - Standard form - Solution of Linear \\nprogramming problem - Basic solution - Basic feasible solution - Degenerate - and Non -\\ndegenerate solutions - Optimal solution - Solution by simplex method - Artificial variables - \\nBig- M method.  \\n \\nModule 2: (9 Hours ) \\nDuality in Linear Programming Problem - Statement of duality theorem - Statement of \\ncomplementary slackness theorem. The primal - Duality solutions using simplex method - \\nRevised simplex method  \\n \\nModule 3:  (9 Hours ) \\nTransportation problem - Solution of Transportation problem - Finding an initial basic feasible \\nsolution - North West Corner method - Matrix minima method - Vogel’s Approximation \\nmethod - Test for Optimality - Modi method - Unbalanced Transportation problem - \\nMaximisation in Transportation problem. Assignment problem - Optimal solution - Hungarian \\nmethod of assignment - Maximization in assignment problem.  \\n \\nModule 4:  (9 Hours ) \\nNetwork analysis - Project scheduling - Construction of project networks - Critical path \\nmethod (CPM) - Identification of critical path using CPM - Estimation of Floats - Total float - \\nIndependent float - Project Evaluation and Review Technique (PERT)  - Computation of \\nexpected completion t imes by PERT.  \\n \\nModule 5:  (9 Hours ) \\nQueuing theory - Elements of Queuing System - Kendall’s notation - Operating characteristics - \\nPoisson process - Exponential distribution - Mean and variance - Birth and Death process. \\nQueuing models based on Poisson process - Single server models with finite and infinite \\ncapacity - Multi server model with finite and infinite capacity.  \\n \\n \\nNote:  \\n• Programming Assignments using Python and appropriate Case Studies may be given at \\nthe end of each module.  \\n \\n• Linear Programming Problems in module 1 and module 2 and Transportation problems \\nin module 3 can be solved using Python library PuLP. Using Numpy, PERT/CPM \\nproblems in module 4 can be solved.  \\nText  Book  \\n1. KantiSwarup, P.K. Gupta and Man Mohan, Operation Resea rch, Sultan Chand (2010)  \\n \\n27 \\n Reference Books  \\n1. Hamdy  A Taha, Operations Research - an introduction, Eighth Edition, Prentice Hall of \\nIndia . \\n2. Ravindran, Philips and Solberg, Wiley, Operation Research, Second edition (2007)  \\nWeb References  \\n1. https://pypi.org/project/PuLP/  \\n2. https://numpy.org/  \\nCourse Contents and Lecture Schedule   \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Module 1  9 Hours  \\n1.1 Linear programming problem - Slack and surplus variable - Standard \\nform  1 \\n \\n1.2 Solution of Linear programming problem - Basic solution - Basic \\nfeasible solution - Degenerate - and Non -degenerate solutions - \\nOptimal solution  2 \\n1.3 Solution by simplex method  3 \\n1.4 Artificial variables - Big- M method  3 \\n2 Module 2  9 Hours  \\n2.1 Duality in Linear Programming Problem  1 \\n2.2 Statement of duality theorem - Statement of complementary \\nslackness theorem  2 \\n2.3 The primal - Duality solutions using simplex method  3 \\n2.4 Revised simplex method  3 \\n3 Module 3  9 Hours  \\n3.1 Transportation problem - Solution of Transportation problem - \\nFinding an initial basic feasible solution - North West Corner method  2 \\n3.2 Matrix minima method - Vogel’s Approximation method  1 \\n3.3 Test for Optimality - Modi method - Unbalanced Transportation \\nproblem - Maximisation in Transportation problem  3 \\n3.4 Assignment problem - Optimal solution - Hungarian method of \\nassignment - Maximization in assignment problem  3 \\n4 Module 4  9 Hours  \\n4.1 Network analysis - Project scheduling - Construction of project \\nnetworks  1 \\n4.2 Critical path method (CPM) - Identification of critical path using \\nCPM  2 \\n4.3 Estimation of Floats - Total float - Independent float  3 \\n4.4 Project Evaluation and Review Technique (PERT)  2 \\n4.5 Computation of expected completion times by PERT  1 \\n28 \\n 5 Module 5  9 Hours  \\n5.1 Queuing theory - Elements of Queuing System - Kendall’s notation - \\nOperating characteristics - Poisson process  1 \\n5.2 Exponential distribution - Mean and variance - Birth and Death \\nprocess  2 \\n5.3 Queuing models based on Poisson process  3 \\n5.4 Single server models with finite and infinite capacity  1 \\n5.5 Multi server model with finite and infinite capacity  2 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n29 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA2 63 CYBER SECURITY  \\n& \\nCRYPTOGRAPHY  ELECTIVE  3 1 0 4 \\n \\nPreamble: This course is designed to provide theoretical  concepts used in cryptography and \\nto introduce the students to various cryptographic algorithms and techniques used for \\nimplementing data security and protection. This course also discusses common web application \\nsecurity vulnerabilities . \\nPrerequisite: Student is expected to have studied mathematics courses that cover Elementary \\nNumber Theory, Finite Field, Discrete Logarithm and Euclidean Algorithm.  \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Explain various types of security attacks, security \\nmechanisms, security services and classical encryption \\ntechniques.  Level 2: \\nUnderstand  \\nCO 2  Make use of Symmetric and Asymmetric encryption \\ntechniques  to solve cryptographic problems.  Level 3:  \\nApply  \\nCO 3  Describe  the concepts of message authentication  codes, \\nhash functions  and digital signing techniques  for ensuring \\nsecure transactions.  Level 2: \\nUnderstand  \\nCO 4  Discuss  security services in Application, Transport and \\nNetwork layers.  Level 2: \\nUnderstand  \\nCO 5  Explain c ommon web application security vulnerabilities  \\nand various prevention mechanisms.  Level 2: \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 1 1    1      \\nCO 2  2 2 2 1   1      \\nCO 3  2 1 1    1      \\nCO 4  2 1 1   2 1      \\nCO 5  2 1 1   2 1      \\n  3/2/1: High/Medium/Low  \\n \\n30 \\n  \\nAssessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2  \\nLevel 1: Remember  15 15 20 \\nLevel 2: Understand  35 35 40 \\nLevel 3: Apply     \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can have a maximum of 2 sub -divisions and carry 6 marks.  \\nSample Course Level Assessment Questions  \\nCourse Outcome 1 (CO 1):  \\n1. Briefly explain each component of OSI security architecture.  \\n2. Compare Substitution and Transposition techniques in cryptography.  \\n3. Explain how steganography  is use d in cryptography.  \\nCourse Outcome 2 (CO 2):  \\n1. Explain block cipher modes of operation.  \\n2. Compare DES and AES  \\n31 \\n 3. Perform encryption and decryption using RSA Algorithm with  \\nparameters: P=17, q = 11, e = 7, M = 88.  \\nCourse Outcome 3 (CO 3):  \\n1. Compare the features of HMAC and CMAC algorithms.  \\n2. Explain important steps in DSS.  \\n3. Describe the terms (a) birthday attack (b) hashc ash (c) blind signature  \\nCourse Outcome 4 (CO 4):  \\n1. Explain any one protocol used in E -mail for security.  \\n2. Explain how security is provided in Network Layer using IPsec.  \\n3. Describe the process of securing electronic transactions.  \\nCourse Outcome 5 (CO 5):  \\n1. Discuss any four Application Security Risks.  \\n2. Which are the different forms of XSS and how to prevent these?  \\n3. Explain the attack scenario of any four web application security vulnerabilities . \\n \\n      Model Question Paper   \\n \\nCourse Code: 20MCA2 63 \\n \\n Course Name: CYBER SECURITY & CRYPTOGRAPHY  \\n \\nMax.  Marks :60        Duration: 3 Hrs  \\nPart A  \\nAnswer all questions.  \\nEach question carries 3 marks (10 x 3 = 30 Marks)  \\n.  \\n1. Compare phishing and ransomware attacks.  \\n2. What is OSI security architecture?  \\n3. List out the advantages and disadvantages of Output Feed Back mode.  \\n4. Explain round functions used in DES.  \\n5. Explain important steps in DSS.  \\n6. Describe the terms (a) birthday attack (b) hashcash (c) blind signature.  \\n7. Describe security association of IPSec.  \\n8. Explain about S/MIME.  \\n32 \\n 9. How can we prevent Injection attack?  \\n10. What is XXE? How to prevent it?                                                                                         \\n                                          \\n (10 x 3=30 marks)  \\n \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n \\n11. Explain Network security model with the help of a neat diagram    (6) \\n                                                                                            \\n       OR \\n \\n12. Describe the working of Playfair cipher and Hill cipher.     (6) \\n \\n13. Apply Diffie -Hellman key exchange algorithm to compute the shared private key using \\nthe values P = 23, g = 9, a= 4, b= 3. Explain the steps in detail.    (6) \\n \\n                                       OR \\n14. Perform encryption and decryption using RSA Algorithm with  \\nparameters: P=17, q = 11, e = 7, M = 88. Explain the steps in detail.   \\n           (6) \\n \\n15. Compare HMAC and CMAC protocol with suitable diagrams .           (6) \\n \\n OR \\n16. Compare various signature schemes with suitable diagrams .    (6) \\n \\n17. Explain PGP cryptographic functions with diagram.      (6) \\n \\n  OR  \\n18.  Explain Secure Electronic Transaction Protocol.      (6) \\n \\n19. Briefly explain any four Application Security Risks.      (6) \\n                        OR \\n20. Explain the attack scenarios of any four web application security vulnerabilities.  (6) \\n                                                                                                              (5 x 6=30 Marks)  \\n \\n \\n \\n \\n33 \\n Syllabus  \\nModule 1: ( 7 Hours)  \\nIntroduction to Cryptography, OSI security architecture:  Security Services, Mechanisms \\nand attacks - Phishing, Ransomware, DoS attack.  Network security model. Classical \\nEncryption techniques - Symmetric cipher model, substitution techniques, transposition \\ntechniques . Steganography.  \\nModule 2: (10 Hours)  \\nConventional Symmetric Key Encryption:  Block ciphers and Stream Ciphers, Block \\nCipher Design Principles, Modes of operation, Data Encryption Standard, Advanced \\nEncryption Standard (AE S), Multiple Encryption, Triple DES . \\nPublic key cryptography:  Principles of public key cryptosystems -The RSA algorithm -Key \\nmanagement – Diffie Hellman Key exchange - Elliptic curve arithmetic - Elliptic curve \\ncryptography . \\nModule 3: ( 10 Hours)  \\nHash Functions and MAC:  Properties of hash functions, birthday attack, hashcash, Message \\nAuthentication Code Algorithms, MAC protocols: HMAC, CMAC . \\nDigital Signatures:  Classification of signature schemes: RSA signature, Digital Signature \\nStandard, Overview of ElGamal and Schnorr schemes, One time signature schemes, Attacks \\non Digital Signatures, Blind Signatures . \\nModule 4:  (10 Hours)  \\nIntroduction to Cyber Security: Email Security:  Security Services for email, Attacks \\npossible through email, Establishing keys privacy, authentication of the source, Message \\nIntegrity, Non -repudiation, Pretty Good Privacy, S/MIM E. \\nIP Security:  Overview of IPSec, IPv4 and IPv6, Authentication Header, Encapsulation \\nSecurity Payload (ESP), Internet Key Exchange . \\nTransport Level Security:  SSL/TLS Basic Protocol, computing the keys, client \\nauthentication, PKI as deployed by SSL, Attacks fixed in v3, Exportability, Encoding, Secure \\nElectronic Transaction (SET) . \\nModule 5: ( 8 Hours)  \\nCommon web application security vulnerabilities:  Injection flaws, Broken authentication, \\nSensitive data exposure, XML External Entities (XXE), Broken access control, Security \\nmisconfiguration, Cross -Site Scripting (XSS), Insecure deserialization, Using components \\nwith known vulnerabilities, Insufficient  logging & monitoring . \\nExample attack scenarios of each of the vulnerabilities listed; how to prevent them  \\n \\n34 \\n Text Book  \\n1. William Stallings, “Cryptography and Network Security,” 6th Edition, Pearson \\nEducation, March (2013).  \\n2. Behrouz A. Forouzan, “Introduction to Cryptography and Network Security”, Tata \\nMcGraw -Hill Publishing 2nd Edition (2011).  \\nReference Books  \\n1. Charlie Kaufman, Radia Perlman and Mike Speciner, “Network Security”, Prentice \\nHall of India, 2002.  \\n2. Manuel Mogollon, “Cryptography and Sec urity Services – Mechanisms and \\nApplications”, Cybertech Publishing, 2008  \\n3. William R. Cheswick, Steven M. Bellovin, Aviel D. Rubin, “Firewalls and Internet \\nSecurity” Addison - Wesley, 2003  \\nWeb Reference s \\n1. http://www.hashcash.org/hashcash.pdf  [Reference for hashcash ]  \\n2. https://owasp.org/www -pdf-archive/OWASP_Top_10 -2017_%28en%29.pdf. \\n[Reference for Module 5 ] \\n3. https://www.coursera.org/learn/crypto  \\n4. https://www.coursera.org/learn/crypto2  \\nCourse Contents and Lecture Schedule  \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Introduction to Cryptography  7 Hours  \\n1.1 What is cryptography, Related Terms, Need of cryptosystems  1 \\n1.2 OSI security architecture: Security Services, Mechanisms  1 \\n1.3 Security attacks - Phishing, Ransomware, DoS attack.   1 \\n1.4 Network security model  1 \\n1.5 Classical Encryption techniques, Symmetric cipher model  1 \\n1.6 Substitution techniques  1 \\n1.7 Transposition techniques, Steganography  1 \\n2 Conventional Symmetric and Public Key Encryption  10 Hours  \\n2.1 Block ciphers and Stream Ciphers, Block Cipher Design Principles  1 \\n2.2 Modes of operation  1 \\n2.3 Data Encryption Standard  1 \\n2.4 Advanced Encryption Standard (AE S) 1 \\n2.5 Multiple Encryption, Triple DES  1 \\n2.6 Public key cryptography : Principles of public key cryptosystems  1 \\n2.7 The RSA algorithm  1 \\n35 \\n 2.8 Key management  1 \\n2.9 Diffie Hellman Key exchange  1 \\n2.10 Elliptic curve arithmetic - Elliptic curve cryptography . 1 \\n3 Hash Functions and MAC  10 Hours  \\n3.1 Properties of hash functio ns, birthday attack  1 \\n3.2 Hashcash, Message Authentication Code Algorithms  1 \\n3.3 MAC protocols: HMAC, CMAC  1 \\n3.4 Digital Signatures: Classification of signature schemes  1 \\n3.5 RSA signature  1 \\n3.6 Digital Signature Standard  1 \\n3.7 Overview of ElGamal and Schnorr schem es 1 \\n3.8 One time signature schemes  1 \\n3.9 Attacks on Digital Signatures  1 \\n3.10 Blind Signatures  1 \\n4 Introduction to Cyber Security  10 Hours  \\n4.1 Email Security: Security Services for email, Attacks possible through \\nemai l 1 \\n4.2 Establishing keys privacy, authentication of the source, Mes sage \\nIntegrity, Non -repudiation  1 \\n4.3 Pretty Good Privacy, S/MIM E 1 \\n4.4 IP Security: Overview of IPSec  1 \\n4.5 IPv4 and IPv6, Authentication Header  1 \\n4.6 Encapsulation Security Payload (ESP), Internet Key Exchange  1 \\n4.7 Transport Level Security: SSL/TLS Basic Protocol  1 \\n4.8 computing the keys, client authentication, PKI as deployed by SSL  1 \\n4.9 Attacks fixed  in v3, Exportability, Encoding  1 \\n4.10 Secure Electronic Transaction (SET)  1 \\n5 Common web application security vulnerabilities  8 Hours  \\n5.1 Common web application security vulnerabilities  1 \\n5.2 Injection flaws, Broken authentication  1 \\n5.3 Sensitive data exposure, XML External Entities (XXE)  1 \\n5.4 Broken access control, Security misconfiguration  1 \\n5.5 Cross -Site Scripting (XSS), Insecure deserialization  1 \\n5.6 Using components with known vulnerabilities, Insufficient logging & \\nmonitoring . 1 \\n5.7 Example attack scenarios of each of the vulnerabilities listed  1 \\n5.8 How to prevent each of the vulnerabilities . 1 \\n \\n36 \\n  \\nCODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA265  Cloud Computing  ELECTIVE  3 1 0 4 \\n \\nPreamble:  The syllabus is prepared with a view to equip the students  to learn basic concepts \\nin cloud computing - compute, storage, networking. They should gain basic understanding of \\norchestration, HA and failover.  \\nPrerequisite: Awareness in Virtualisation and Containers is desirable.  \\nCourse Outcomes:   After completion of  the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Understand  the basic concepts in cloud computing and \\nOpenStack logical architecture  Level 2: \\nUnderstand  \\nCO 2  Discuss OpenStack cloud controller and common services  Level 3:  \\nApply  \\nCO 3  Compare different OpenStack compute service components \\nand storage types  Level 2: \\nUnderstand  \\nCO 4  Describe the OpenStack Networking - Connection types and \\nnetworking services  Level 2: \\nUnderstand  \\nCO 5  Discuss orchestration, HA and failover in OpenStack  Level 2:  \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2      3  2     1 \\nCO 2  2    3  2     1 \\nCO 3  2    3  2     1 \\nCO 4  2    3  2     1 \\nCO 5  2    3  2     1 \\n  3/2/1: High/Medium/Low  \\n37 \\n  \\nAssessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2 \\nLevel 1: Remember  20 15 20 \\nLevel 2: Understand  20 35 30 \\nLevel 3: Apply  10  10 \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\n \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance       : 8 marks  \\nContinuous Assessment Test (2 numbers)   : 20 marks  \\nAssignment/Quiz/Course project    : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can ha ve a maximum 2 subdivisions and carry 6 marks.  \\nSample Course Level Assessment Questions   \\nCourse Outcome 1 (CO 1):  \\n1. List and explain various components of Nova compute service.  \\n2. Explain the neutron architecture?  \\n3. Briefly describe  keystone identity management.   \\n38 \\n Course Outcome 2 (CO 2):  \\n1. Explain the telemetry services in OpenStack.  \\n2. Explain the steps involved in bringing  up a working OpenStack Ansible on the \\ndeployment host.  \\n3. Explain the steps in network configuration  \\nCourse Outcome 3 (CO 3):  \\n1. Explain briefly swift architecture  \\n2. Briefly explain how data is handled in the cluster by swift  \\n3. What is meant by CPU over  commitment?  \\nCourse Outcome 4 (CO 4):  \\n1. Explain steps in associating a floating IP to a virtual machine.  \\n2. Briefly explain the steps in creating a virtual network with two subnets  \\n3. Briefly explain Linux bridge -based connectivity?   \\nCourse Outcome 5 (CO 5):  \\n1. Briefly explain the major components in heat?  \\n2. Explain the different metrics that can be measured in a highly available infrastructure?  \\n3. Explain the n eed for Service  level agreement.  \\n \\n      Model Question Paper   \\nCourse Code: 20MCA265  \\n Course Name: Cloud Computing  \\n \\nMax.  Marks :60          Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1. What are the different components  in OpenStack logical architecture?  \\n2. Differentiate between private cloud and public cloud.  \\n3. Explain asymmetric clustering and symmetri c clustering.  \\n4. List out the functionalities handled by the cloud  controller.  \\n5. Briefly explain docker containers.  \\n6. Compare object storage with NAS/SAN based storage.  \\n7. Describe the steps in connecting two networks using a virtual router.  \\n8. Write a short note on fi rewall as a service  \\n9. List the HA levels in OpenStack.  \\n10. Explain the purpose of HA proxy.  \\n39 \\n Part B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n11 List and explain the different components in Open Stack Architecture.  6 \\n  OR   \\n12 a. Explain the provisioning of VM in OpenStack using a diagram  4 \\n  b. Describe the best practices used in Physical  mode design  2 \\n    \\n13 Explain the keystone architecture  6 \\n    OR   \\n14 Explain the steps involved in running Open Stack playbooks  6 \\n   \\n15 Explain in detail the multiple services involved in launching an instance  6 \\n    OR   \\n16 Explain the steps in deploying swift service  6 \\n   \\n17 Explain the architecture of neutron in detail  6 \\n    OR   \\n18 Explain the categorization of neutron virtual networks in detail  6 \\n   \\n19 Explain stacking in OpenStack  6 \\n    OR   \\n20 Explain in detail steps involved in setting a database with high availability  6 \\nSyllabus  \\nModule 1: Overview of OpenStack (7 Hours)  \\n \\nIntroduction to cloud computing, private cloud, public cloud, hybrid cloud architecture.  \\nCloud Services - Infrastructure as a Service, Platform as a Service, Storage as a Service.  \\nDesigning OpenStack Cloud Architectural Consideration - OpenStack - The new data centre \\nparadigm - OpenStack logical architecture  - Nova - Compute Service -Neutron - Networking \\nservices - Gathering the pieces and building a picture - A sample architecture setup . \\n \\nModule 2: OpenStack cluster - Controller and common services (6 Hours)  \\nOpenStack Cluster   – The Cloud Controller and Common Services - Asymmetric clustering, \\nSymmetric clustering, The cloud controller - The keystone service . \\nThe nova -conductor service, The nova -scheduler service, The API services, Image \\nmanagement, The networ k service, The horizon dashboard, The telemetry services . \\n40 \\n Module 3: OpenStack compute and Storage (12 Hours)  \\nOpenStack Compute -The compute service components  - Deciding on the hypervisor  - \\nOpenStack Magnum  Project  - Segregating the compute cloud  - Overcommitment \\nconsiderations  - Storing instances\\' alternatives  - Understanding instance booting  - Planning \\nfor service recovery . \\nOpenStack Storage - Block, Object, and File Share  - Understanding the storage types  - \\nEphemeral Storage  - Persistent storage  - A spotlight on Swift  - Deploying Swift service  - \\nUsing block storage service: Cinder . \\nModule 4:  OpenStack Networking (10 Hours)  \\nThe architecture of Neutron  - Implementing virtual networks  - Connecting virtual networks \\nwith routers  - Implementing network security in OpenStack.  \\nOpenStack Networking - The architecture of Neutron - Implementing virtual networks - \\nVLAN, Tunnel based, Virtual Switches, The ML2 Plugin. Neutron Subnets - Connecting \\nvirtual networks with routers - Configuring the routing service - connecting networks us ing \\na virtual router, connecting to the external world, connectivity from the external world, \\nassociating a floating IP  - Implementing network security in OpenStack  \\nModule 5: OpenStack Orchestration, HA and failover (10 Hours)  \\nOrchestration in OpenStack - Heat and its Components, stacking in OpenStack, Open Stack \\nOrchestration with Terraform. Open Stack HA and failover:  Scope of HA in OpenStack, HA \\nin the database, HA in the Queue, Implementing HA on RabbitMQ . \\nText  Book  \\n1. Omar Khedher , Chandan Datta Chowdhury, Mastering OpenStack, 2nd Edition, Packt \\nPublishing, 2017  \\nReference  Books  \\n1. Tom Fifield, Diane Fleming, Anne Gentle,Lorin Hochstein, Jonathan Proulx, Everett \\nToews, and Joe Topjian,  OpenStack Operations Guide, O\\'REILY,1/e, 2014.  \\n2. Uchit Vyas, Applied OpenStack Design Patterns, Apress, 1/e, 2016.  \\n3. V. K. Cody Bumgardner, OpenStack in action, Manning, 2016.  \\n4. Amar Kapadia, Sreedhar Varma, Kris Rajana , Implementing Cloud Storage with \\nOpenStack Swift, Packt Publishing, 2014.  \\n5. https://docs.openstack.org/wallaby/?_ga=2.231002015.1428061357.1620834394 -\\n1139122985.1620834394  \\n \\n \\n \\n \\n41 \\n Course Contents and Lecture Schedule  \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Overview of OpenStack  7 Hours  \\n1.1 Introduction to cloud computing, private cloud, public cloud, hybrid \\ncloud architecture.  1 \\n \\n1.2 Cloud Services - Infrastructure as a Service, Platform  as a Service, \\nStorage  as a Service  1 \\n1.3 Designing OpenStack Cloud Architectural Consideration - OpenStack \\n- The new data center paradigm -OpenStack logical architecture  1 \\n1.4  Nova - Compute service  1 \\n1.5 Neutron - Networking services  1 \\n1.6 Gathering the pieces and building a picture  1 \\n1.7 A sample architecture setup  1 \\n2 OpenStack cluster - Controller and common services  6 Hours  \\n2.1 OpenStack Cluster – The Cloud Controller and Common Services - \\nAsymmetric clustering, Symmetric clustering  1 \\n2.2 The cloud controller - The keystone service  2 \\n2.3 The nova -conductor service, The nova -scheduler service, The API \\nservices, Image management.  1 \\n2.4 The network service, The horizon dashboard, The telemetry services  2 \\n3 OpenStack compute and Storage  12 Hours  \\n3.1 The compute service components -Deciding on the hypervisor -\\nOpenStack Magnum project  1 \\n3.2 Segregating the compute cloud  1 \\n3.3 Overcommitment considerations  1 \\n3.4 Storing instances\\' alternatives  1 \\n3.5 Understanding instance booting  1 \\n3.6 Planning for service recovery  1 \\n3.7 OpenStack Storage - Block, Object, and File Share -Understanding \\nthe storage types  1 \\n3.8 A spotlight on swift  2 \\n3.9 Deploying swift service  1 \\n42 \\n 3.10 Using Block Storage Service Cinder  2 \\n4 OpenStack Networking  10 Hours  \\n4.1 The architecture of Neutron  1 \\n4.2 Implementing virtual networks - VLAN, Tunnel based  1 \\n4.3 Virtual Switches, The ML2 Plugin  1 \\n4.4 Neutron Subnets  2 \\n4.5 Connecting virtual networks with routers  - Configuring the routing \\nservice  1 \\n4.6 Connecting networks using a virtual router, Connecting to the \\nexternal world  1 \\n4.7 Connectivity from the external world, Associating a floating IP  1 \\n4.8 Implementing network security in OpenStack  2 \\n5 OpenStack Orchestration, HA and Failover  10 Hours  \\n5.1 Orchestration in OpenStack, Heat and its Components  1 \\n5.2 Stacking in OpenStack  2 \\n5.3 Open Stack Orchestration with Terraform  2 \\n5.4 Scope of HA in OpenStack  2 \\n5.5 HA in the database  1 \\n5.6 HA in the Queue, Implementing HA on RabbitMQ  2 \\nSuggested Assignments  \\n1) Create VMs in your physical machine using OpenStack to set up the following services: \\nMoodle, MySQL Server, Samba. Design the desired configuration of the  physical \\nmachine to handle the requir ements of the entire college.  \\n2) Set up storage services for storing external files for Moodle.  \\n3) Set up firewall rules for samba,  MySQL server, allow the connection to MySQL server \\nonly to Moodle VM. \\n4) Set up recovery plans for the above services  \\n5) Convert the MySQL server to HA MySQL server.  \\n6) Setup a load balancer for the Moodle server.  \\n \\n \\n \\n43 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA267  CYBER FORENSICS  ELECTIVE  3 1 0 4 \\n \\nPreamble: This course helps the learner to understand the fundamentals of cyber forensics. \\nStudent will learn common approaches, practices and techniques used for collecting and \\npreserving digital evidences in this course.   \\nPrerequisite: Basic knowledge in operating systems & computer networ ks. \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Explain a computer crime and the concept of rules or policy \\nviolations.  Level 2: \\nUnderstand  \\nCO 2  Gather evidences and preserve the collected evidence with \\nthe required knowledge on various storage format choices.  Level 3:  \\nApply  \\nCO 3  Describe  digital storage and file systems and extract data \\nusing Autopsy.  Level 3:  \\nApply  \\nCO 4  Explain  mobile device forensics  and practice data \\nacquisition procedures for network forensics  using \\nWireshark.  Level 3:  \\nApply  \\nCO 5  Prepare forensics reports both using tools and manually and \\nexplain ethics and code for expert witness.  Level 2: \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 1  1  2 1  2    \\nCO 2  2 1  1 2  1      \\nCO 3  2 1  1 2  1      \\nCO 4  2 1  1 2  1      \\nCO 5  2 1  1 2 3 1  1    \\n  3/2/1: High/Medium/Low  \\n \\n \\n \\n44 \\n Assessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2  \\nLevel 1: Remember  15 15 20 \\nLevel 2: Understand  35 35 40 \\nLevel 3: Apply     \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination (ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10  compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can have a maximum of 2 sub -divisions and carry 6 marks.  \\nSample Course Level Assessment Questions  \\nCourse Outcome 1 (CO 1):  \\n1. How to perform industrial espionage investigation?  \\n2. Explain the various steps involved in cyber forensics investigation.  \\n3. Identify real -time scenarios that is considered as company policy violation.  \\nCourse Outcome 2 (CO 2):  \\n1. What are the advantages  of proprietary evidence format over row format?   \\n2. Differentiate static acquisitions from live acquisitions . \\n3. How to ensure the integrity of collected digital evidence? List the techniques employed \\nto validat e the collected evidence.  \\n45 \\n  \\nCourse Outcome 3 (CO 3):  \\n1. Describe the various activities involved while starting a windows operating system.  \\n2. Explain the various file system organization used in windows operating system.  \\n3. What is the importance of windows registry analysis in forensic investigation?  \\n4. How to ensure the integrity of collected evidences?  \\n5. Differentiate the forensics procedure in Linux and MacOS.  \\nCourse Outcome 4 (CO 4):  \\n1. Explain the mobile forensics procedure in detail.  \\n2. Illustrate the use of Wire shark packet analyser.  \\n3. How the forensics acquisition method in mobile differ s from that in computer system?  \\nCourse Outcome 5 (CO 5):  \\n1. How to write an investigation report that can sustain in court of law?  \\n2. Discuss how Autopsy tool is used to generate forensics report.  \\n3. Address the difficulties that occurred while preparing an Expert Testimony.  \\n \\n \\n      Model Question Paper   \\nCourse Code: 20MCA267  \\n \\n Course Name: Cyber Forensics  \\n \\nMax. Marks :60              Duration: 3 Hrs  \\nPart A  \\nAnswer all questions.  \\nEach question carries 3 marks (10 x 3 = 30 Marks)  \\n \\n1. Categorize  the formats used to store  the collected digital evidences.  \\n2. What do you mean by a computer crime? Which activities are considered as company       \\n policy violations?  \\n3.  Enumerate the features of Resilient File System.  \\n4.  Write down the operations involved in boot sequence.  \\n5.  Differentiate between soft link and hard link.  \\n6.  Which are the data  acquisition tools available in Linux that is forensically sound?  \\n7.  List the features of  Wireshark tool.  \\n46 \\n 8.  List different types of mobile forensic acquisition procedures.  \\n9. State the guidelines for writing a report which is admissible in a court of law.  \\n10. What a re the different types of forensics reports?                                                                                            \\n                                          \\n (10 x 3=30 marks)  \\n \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n \\n11. What is  evidence bag? Describe standard operating procedures for securing evidence \\nbefore transporting it to forensic lab.                                                                          (6) \\n       OR \\n \\n12.  How the retention policy of evidence related to evidence storage mediums?       (6) \\n \\n \\n13. Explain the i mportance of Windows Registry in forensics analysis.     (6) \\n \\n                                       OR \\n14. What is a solid -state storage device? Discuss the usage of Microsoft  BitLocker tool.\\n             (6) \\n \\n15. Explain the file structures of Linux and MacOS.                             (6) \\n \\n OR \\n16.  Define write blocker? Expla in the use of Sleuth Kit tool.      (6) \\n \\n17. Explain the standard operating procedures used for mobile forensics.    (6) \\n \\n  OR  \\n18.  Discuss the standard operating procedures used for network forensics .   (6) \\n \\n19.  List and explain  the steps involved in generating forensics report using Autopsy tool.\\n             (6) \\n                        OR \\n20. What are the responsibilities of a Computer Forensic Investigator?     (6) \\n   \\n                                                                                                              (5 x 6=30 Marks)  \\n \\n \\n \\n47 \\n Syllabus  \\nModule 1: (8 Hours)  \\nOverview of computer crime, Overview of company policy violation, Preparing a case - \\nPlanning an investigation, Securing evidence. Industrial espionage investigation.  \\nConducting an investigation: Gathering evidence, Bit -stream copy of evidence.   \\nStorage formats for storing collected d igital evidence - Raw format, Proprietary formats, \\nAdvanced Forensic Format (AFF). Acquisition tools and methods. Digital evidence \\nvalidation methods and tools .  \\nStoring Digital evidence - Evidence Retention.  \\nFamiliarizing Autopsy for Windows - a free foren sics tool.  \\nModule 2: (10 Hours)  \\nUnderstanding Digital data and storage systems: Understanding boot sequence, \\nUnderstanding Disk Drives - Solid -state Storage Devices (SSDs).  \\nMicrosoft File Systems - Disk partitions, Understanding FAT, Understanding NTFS, MFT - \\nfile attributes, file data, NTFS compressed files, NTFS encrypted file system, Deleting NTFS \\nfile system, ReFS.  \\nWhole disk encryption, Microsoft BitLocker. Understanding  Windows Registry. Microsoft \\nWindows startup tasks.  \\nA practical assignment may be given in encrypting a partition of your computer hard disk \\ndrive/ encrypting USB flash drive to avoid firm -level attack.  \\nModule 3: (10 Hours)  \\nLinux file structures - File structures in Ext4, Hard links and Symbolic links.  \\nMacintosh (MacOS) file structures - Forensic procedures in MacOS.  \\nSetting up Sleuth Kit and Autopsy - Examining a case with Sleuth Kit and Autopsy, \\nImportance of Write -blocker . \\nAcquiring data with a Linux boot CD - Preparing a target drive for data acquisition, Using \\ndd and dcfldd commands.  \\nValidating data acquisitions - Linux validation methods, Windows validation methods.  \\nFollowing practical assignments may be given:  \\ni. Recover deleted files from pen drive  \\nii. Extract camera information from recovered images  \\niii. Extract deleted internet browsing history  \\niv. Recover deleted files from unallocated space using Autopsy  \\nModule 4:  (10 Hours)  \\nUnderstanding Mobile Device forensics - Mobile phone basics, Understanding Mobile phone \\nhardware.  \\n48 \\n Acquisition procedures for Mobile devices, Mobile Forensic equipment, SIM card readers, \\nMobile phone Forensics tools and methods.  \\nNetwork Forensics - The Need for Established Procedures, Securing  a Network, Developing \\nProcedures for Network Forensics, Wireshark packet analyser.  \\nPractical assignments may be given:  \\ni. Identify students who use college lab facility to browse shopping websites  \\nii. Identify the hacking attempt on a closed port using ping sweep  \\niii. Using Wireshark  retrieve the username and password of users  who browse less \\nsecure website with Wi- Fi connection  \\nModule 5: (7 Hours)  \\nUnderstand the importance of Forensics Reports, Types of reports, Guidelines for writing \\nreports, Layout and presentation of reports, Generating reports with Autopsy.  \\nEthics and codes for Expert Witness - Forensics Examiner’s role in testifying, Considerations \\nin disqualification, Determining admissibility of evidence. Ethical difficulties in Expert \\nTestimony, Et hical responsibilities.  \\nText Book  \\n1. Bill Nelson, Amelia Phillips, Christopher Steuart, “Guide to Computer Forensics and \\nInvestigations”, Cengage Learning, 6th Edition.  \\nReference Books  \\n1. Marjie T. Britz , “Computer Forensics and Cyber Crime”, Pearson Third Edition 2013.  \\n2. Marie - Helen Maras “Computer Forensics: Cybercriminals, Laws, and Evidence”, \\nJones & Bartlett Learning, Second Edition 2015.  \\nWeb References  \\n1. https://www.wireshark.org/download/docs/user -guide.pdf (Reference for Wireshark)  \\n2. http://www.open.edu/openlearn/futurelearn/cyber -security  \\n \\n \\n \\n \\n \\n \\n \\n \\n49 \\n Course Contents and Lecture Schedule  \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Module 1  8 Hours  \\n1.1 An overview of computer crime s and c ompany policy violation s 1 \\n \\n1.2 Preparing a case - Planning an investigation, Securing evidence. \\nIndustrial espionage investigation  1 \\n1.3 Conducting an investigation: Gathering evidence, Bit -stream copy of \\nevidence  1 \\n1.4 Storage formats for storing collected digital evidence - Raw format, \\nProprietary formats, Advanced Forensic Format (AFF)  1 \\n1.5 Acquisition tools and methods  1 \\n1.6 Digital evidence validation methods and tools  1 \\n1.7 Storing Digital evidence -Evidence Retention  1 \\n1.8 Familiarizing Autopsy for Windows - a free forensics tool  1 \\n2 Module 2  10 Hours  \\n2.1 Understanding Digital data and storage systems, Understanding boot \\nsequence  1 \\n2.2 Understanding Disk Drives  1 \\n2.3 Solid -state Storage Devices (SSDs)  1 \\n2.4 Microsoft File Systems - Disk partitions  1 \\n2.5 Understanding FAT  1 \\n2.6 Understanding NTFS, MFT - file attributes, file data  1 \\n2.7 NTFS compressed files, NTFS encrypted file system  1 \\n2.8 Deleting NTFS file system, ReFS  1 \\n2.9 Whole disk encryption, Microsoft BitLocker  1 \\n2.10 Understanding Windows Registry , Microsoft Windows startup tasks  1 \\n3 Module 3  10 Hours  \\n3.1 Linux file structures - File structures in Ext4  1 \\n3.2 Hard links and Symbolic links  1 \\n3.3 Macintosh (MacOS) file structures - Forensic procedures in MacOS  1 \\n3.4 Setting up Sleuth Kit and Autopsy - Examining a case with Sleuth Kit \\nand Autopsy  1 \\n3.5 Importance of Write -blocker  1 \\n50 \\n 3.6 Acquiring data with a Linux boot CD  1 \\n3.7 Preparing a target drive for data acquisition  1 \\n3.8 Using dd and dcfldd  commands  1 \\n3.9 Validating data acquisitions - Linux validation methods  1 \\n3.10 Windows validation methods  1 \\n4 Module 4  10 Hours  \\n4.1 Understanding Mobile Device forensics - Mobile phone basics  1 \\n4.2 Understanding Mobile phone hardware  1 \\n4.3 Acquisition procedures for Mobile devices  1 \\n4.4 Mobile Forensic equipment  1 \\n4.5 SIM card readers  1 \\n4.6 Mobile phone Forensics tools and methods  1 \\n4.7 Network Forensics - The Need for Established Procedures  1 \\n4.8 Securing  a Network  1 \\n4.9 Developing Procedures for Network Forensics  1 \\n4.10 Wireshark packet analyser  1 \\n5 Module 5  7 Hours  \\n5.1 Understand the importance of Forensics Reports, Types of reports  1 \\n5.2 Guidelines for writing reports, Layout and presentation of reports  1 \\n5.3 Generating reports with Autopsy  1 \\n5.4 Ethics and codes for Expert Witness - Forensics Examiner’s role in \\ntestifying  1 \\n5.5 Considerations in disqualification, Determining admissibility of \\nevidence  1 \\n5.6 Ethical difficulties in Expert Testimony  1 \\n5.7 Ethical responsibilities  1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n51 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA269  COMPILER DESIGN  ELECTIVE  3 1 0 4 \\n \\nPreamble:  The objective of this course is to explore the principles, algorithms and data \\nstructures involved in the design of compilers. It includes lexical analysis, parsing techniques, \\ngenerating grammars, intermediate code generation, code optimization and code g eneration \\nphases.  \\nPrerequisite: Nil \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO \\nNo. Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Explain different phases of compiler and perform  lexical analys is \\nusing the concepts of regular expressions and finite automata.   Level 2: \\nUnderstand  \\nCO 2  Develop  top down  and bottom -up parsers to perform syntax \\nanalysis  using context free grammar.  Level 3:  \\nApply  \\nCO 3  Explain  syntax directed translation schemes  and type checking \\nfor a given grammar . Level 2:  \\nUnderstand  \\nCO 4  Distinguish different intermediate code representations and \\ngenerate  intermediate code for statements in high level languages.  Level 2: \\nUnderstand  \\nCO 5  Describe  various code optimization techniques and generate \\nmachine dependent code .  Level 2:  \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 2  -   -  - 2  -  - -  - - \\nCO 2  2 2 2 - - - 2 - - - - - \\nCO 3  2 2  - - - 2 - - - - - \\nCO 4  2  1  -  - - 2 - - - - - \\nCO 5  2 3   - - - 2 - - - - - \\n  3/2/1: High/Medium/Low  \\n \\n52 \\n Assessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End Semester \\nExamination  \\n1 2  \\nLevel 1: Remember  15 15 20 \\nLevel 2: Understand  25 35 30 \\nLevel 3: Apply  10  10 \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\n Mark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance       : 8 marks  \\nContinuous Assessment Test (2 numbers)   : 20 marks  \\nAssignment/Quiz/Course project    : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A  and Part B . Part A \\ncontain 10 questions with 2 questions from each module, having 3 marks for each question. \\nStudents should answer all questions. Part B contains 2 questions from each module of which \\nstudent should answer any one . Each ques tion carries 6 marks.  \\n Sample Course Level Assessment Questions  \\n  Course Outcome 1 (CO 1):  \\n1. Explain how the regular expressions and finite automata are used for specification and \\nrecognition of tokens.  \\n2. State the role of lexical analy ser. Identify the lexemes and their corresponding tokens \\nin the following statement: printf (“Simple Interest=%f \\\\n”, si);  \\n3. Draw the DFA for the regular expression (a | b) * (abb | a+ b).  \\n4. Trace the output after each phase of the compiler for the assignment st atement:  \\n            a = b + c * 10, if variables given are of float type.  \\n53 \\n  \\nCourse Outcome 2 (CO 2):  \\n1. Find the LR (0) items for the grammar  \\n         S->SS | a| €.  \\n2. Show the steps involved in recursive descent parsing with backtracking for the string \\ncad wit h the given grammar: S -> cAd, A -> ab | a  \\n3. Construct the predictive parsing table for the following grammar:  \\n         S -> (L) | a   \\n         L -> L,S | S  \\n \\nCourse Outcome 3 (CO 3):  \\n1. Write  the S -attributed SDD of a simple desk calculator and show annotated parse tree \\nfor the expression (3+4) *(5+6).  \\n2. Explain bottom - up evaluation of S - attributed definitions.  \\n3. Explain the specification of a simple type checker  \\n \\nCourse Outcome 4 (CO 4):  \\n1. Draw  DAG for the expression (a/10 + (b -10)) *(a/10 + (b -10)). Also write the sequence \\nof instructions used for the DAG construction.  \\n2. Write the three -address code sequence for the statement x=y*z + y* -z. Also give its \\ntriple representation.  \\n3. Write syntax directe d definitions to construct syntax tree and three address code    for \\nassignment statements.  \\nCourse Outcome 5 (CO 5):  \\n1. Using code generation algorithm generate code sequence for the expression x = (a – b) \\n+ (a + c).  \\n2. With suitable example of a basic block, e xplain the code -improving transformations of \\na basic block.  \\n3. Explain common sub expression elimination with an example.  \\n \\n \\n \\n \\n \\n \\n54 \\n Model Question Paper   \\nCourse Code: 20MCA269  \\n \\n Course Name: COMPILER DESIGN  \\n                                                   \\nMax.  Marks :60        Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1 State the role of lexical analyzer . Identify the lexemes and their \\ncorresponding tokens in the following statement: printf (“Simple \\nInterest=%f \\\\n”, si);  (3) \\n2 Draw the transition diagram for the regular definition,  \\nrelop  → < | <= | = | <> | >= | >  (3) \\n3 Find the FIRST and FOLLOW of the non -terminals in the grammar  \\nS->  aABe  \\nA->  Abc|b  \\nB->  d (3)  \\n4 Demonstrate the identification of handles in operator precedence parsing?  (3) \\n5 What is a Syntax Directed Definition? Show an example.  (3) \\n6 Distinguish between synthesized and inherited attributes.  (3) \\n7 Write the three -address code sequence for the statement x=y*z + y* -z. Also \\ngive its triple representation.  (3) \\n8 Discuss about the getreg () function in code generator algorithm  (3) \\n9 Identify any two issues in the design of a Code Generator.  (3) \\n10 Explain common sub  expression elimination with an example.  (3) \\nPart B  \\nAnswer all questions. Each question carries  6 marks . (5 * 6 = 30 Marks)  \\n11  Explain the working of different phases of a compiler. Illustrate with a \\nsource language statement.  (6) \\n  OR  \\n55 \\n 12  Explain how the regular expressions and finite state automata are used \\nfor the specification and recognition of tokens?  (6) \\n13  Construct LALR parse table for the grammar      \\nS->C \\nC->cC|d  (6) \\n  OR  \\n14  Design a recursive descent parser for the grammar  \\nE->E + T | T  \\nT->T*F | F  \\nF->(E) | id  (6) \\n15  Differentiate between S -attributed and L -attributed definitions with \\nsuitable examples  (6) \\n  OR  \\n16  Write the S yntax Directed Definition  for a simple type declaration and \\ndraw the annotated parse tree for the declaration float a, b, c.  (6) \\n17  Explain  the following and show an example for each.  \\ni). Three -address code iii). Triples  \\nii). Quadruples iv). Indirect triples  (6) \\n  OR  \\n18  Write S yntax Directed Definition  to produce three -address code for \\nBoolean expressions and obtain the three -address code for the statement \\ngiven below:  \\n   while a < b do  \\n                    if c < d then  \\n                              x = y + z  \\n                         else \\n                              x = y – z (6) \\n19  Explain different code optimization techniques available in local and \\nglobal optimizations?  (6) \\n  OR  \\n20  Write the code generation algorithm. Using this algorithm generate code \\nsequence for the expression x = (a – b) + (a + c).  (6) \\n \\n \\n56 \\n Syllabus  \\nModule 1 (8 Hours)  \\nIntroduction to compilers: Analysis of the source program, Phases of a compiler, Grouping \\nof phases  \\nLexical analysis: role of lexical analyser, input buffering, specification of tokens, recognition \\nof tokens, Deterministic and Non -Deterministic Finite automata, Regular expression to NFA \\nand DFA  \\nModule 2  (12 Hours)  \\nSyntax analysis: Role of parser, Context free grammars  \\nTop down parsing: Recursive Descent parsing, Predictive parsing, LL(1) Grammars.  \\nBottom -up parsing: Shift Reduce Parsing, Operator Precedence Parsing (concepts only),  \\nLR parsing – Constructing SLR parsing tables, Constructing  Canonical LR parsing tables \\nand Constructing LALR parsing tables.  \\nModule 3 ( 8 Hours)  \\nSyntax directed translation: Syntax directed definitions, Bottom -up evaluation of S - \\nattributed definitions, L - attributed definitions, Top -down translation, Bottom -up evaluation \\nof inherited attributes.  \\nType Checking: Type systems, Specification of a simple type checker.  \\nModule 4  (7 Hours)  \\nIntermediate code generation: Graphical representations, Three address code  - Quadruples -\\ntriples  - Indirect triples, Assignment Statements, Boolean Expressions, Control flow \\nstatements  \\nModule 5 (10 Hours)  \\nCode Optimization: Principal sources of optimization, Optimization of Basic blocks, Global \\ndata flow analysis . \\nCode generation: Issues in the design of a code generator, The target machine, Basic blocks \\nand flow graphs, A simple code generator, Peephole optimization . \\nNote : Programming assignment s using lexical analyser generator , using parser generator.  \\n \\nText Books  \\n1. Alfred V.Aho , Monica S. Lam, Ravi Sethi, Jeffrey D. Ullman , Compilers – Principles , \\nTechniques and Tools, Addison Wesley, 2nd  Edition, 2006.  \\n \\n57 \\n Reference Books  \\n1. V Raghavan - Principles of Compiler Design – Tata McGraw Hill,  2nd edition ,2011  \\n2. Jean Paul Tremblay and Sorenson., The Theory and Practice of Compiler Writing \\n,McGraw Hill,2nd Edition,2006  \\n3. Nandini Prasad, Principles of compiler design, Elsevier, 2nd Edition,2012  \\n4. Kenneth C. Louden, Compiler Construction -Principles and Practice, 2nd Edit ion, \\nCengage, 2010.  \\n5. Keith Cooper and Linda Torczon,  ͞Engineering a Compile r͟, 2nd Edition, Elsevier, 2011  \\n \\nCourse Contents and Lecture Schedule   \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Lexical Analysis  8 Hours  \\n1.1 Lexical Analysis: Analysis of the source program  1 \\n1.2 Phases of a compiler, Grouping of phases  \\n 1 \\n1.3 Lexical analysis: role of lexical analyser, input Buffering  1 \\n1.4 specification of tokens, recognition of tokens  1 \\n1.5 Deterministic and Non -Deterministic Finite automata  2 \\n1.6 Regular expression to NFA and DFA  2 \\n2 Syntax Analysis  12 Hours  \\n2.1 Syntax analysis: Role of parser, Context free grammars  1 \\n2.2 Top-down parsing: Recursive Descent parsing  2 \\n2.3 Predictive parsing, LL(1) Grammars  2 \\n2.4 Bottom -up parsing: Shift Reduce Parsing  1 \\n2.5 Operator Precedence Parsing  \\n 1 \\n2.6 LR parsing – Constructing SLR parsing tables  2 \\n2.7 Constructing Canonical LR parsing tables  2 \\n2.8 Constructing LALR parsing tables.  1 \\n58 \\n  \\n3 Syntax directed translation and Type Checking  8 Hours  \\n3.1 Syntax directed translation: Syntax directed definitions  1 \\n3.2 Bottom - up evaluation of S attributed definitions, L - attributed \\ndefinitions  2 \\n3.3 Top-down translation, Bottom -up evaluation of inherited \\nattributes.  \\n 2 \\n3.4 Type Checking: Type systems  1 \\n3.5 Specification of a simple type checker.  \\n 2 \\n4 Intermediate code generation  7 Hours  \\n4.1 Intermediate code generation: Graphical representations  2 \\n4.2 Three address code -quadruples -triples -Indirect triples  2 \\n4.3 Assignment Statements, Boolean Expressions, Control flow \\nstatements  2 \\n4.4 Control flow statements  1 \\n5 Code Optimization and Code Generation  10 Hours  \\n5.1 Code Optimization: Principal sources of optimization  2 \\n5.2 Optimization of Basic blocks,  \\n 1 \\n5.3 Global data flow analysis  2 \\n5.4 Code generation: Issues in the design of a code generator.  2 \\n5.5 The target machine, Basic blocks and Flow graphs  2 \\n5.6 Peephole optimization  1 \\n \\n \\n \\n \\n59 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA281  INTERNET OF THINGS  ELECTIVE  3 1 0 4 \\n \\nPreamble: This course intends to provide insight into new innovations that will build novel \\ntype of interactions among things and humans, and enables the realization of smart cities, \\ninfrastructures, and services for enhancing the quality of life and utilization of resources. An \\noverview of IOT and its related concepts, different IOT architectures and their components, \\nemerging parad igms such as Fog computing, Platforms and solutions supporting development \\nand deployment of IOT applications, message passing mechanisms such as RPC, REST, and \\nCoAP, data and knowledge management, data confidentiality, data integrity, and operation \\ncontro l issues faced by IOT are included in the course.   \\n \\nPrerequisite: Basic concepts of Information Technology and Internet.  \\nCourse Outcomes: After completion of the course the student will be able to  \\nCO No:  Course Outcome (CO)  Blooms Category \\nLevel  \\nCO 1  Describe the main concepts and features of the IOT paradigm.  Level 2: \\nUnderstand  \\nCO 2  Discuss Fog computing, TinyOS - nesC and  programming \\nframeworks for IOT Level 2:  \\nUnderstand  \\nCO 3  Describe the data management techniques applied to the IOT \\nenvironment.  Level 2  \\nUnderstand  \\nCO 4  Explain security, and privacy in IOT environments  Level 2  \\nUnderstand  \\nCO 5  Discuss  key enablers and solutions to enable practical IoT \\nsystems  Level 2  \\nUnderstand  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3      2      \\nCO 2  3 1     2      \\nCO 3  3 1     2      \\nCO 4  3 1     2      \\nCO 5  3 1 1    2      \\n 3/2/1: High/Medium/Low  \\n \\nAssessment Pattern  \\nBloom’s Category  Continuous Assessment \\nTests  End Semester Examination  \\n1 2 \\nRemember  20 20 20 \\nUnderstand  30 30 40 \\n60 \\n Apply     \\nAnalyse     \\nEvaluate     \\nCreate     \\n \\nMark Distribution  \\nTotal \\nMarks  CIE ESE  ESE \\nDuration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontain 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can hav e a maximum of 2 subdivisions and carry 6 marks.  \\n \\nCourse Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n1. Compare SOA -based architecture and API -oriented architecture.  \\n2. Neatly sketch the open IOT architecture for IOT/CLOUD convergence.  \\n3. List and explain the applications of device/cloud collaboration.  \\nCourse Outcome 2 (CO2)  \\n1. What are the advantages associated with Fog computing?  \\n2. Comment on the four broad requirements that motivate the design of TinyOS.  \\n3. Summarize the communication paradigms and technologies used in resource -\\n constrained environments.  \\nCourse Outcome 3(CO3):  \\n1. Explain stream and stream processing in IOT.  \\n2. Write and explain the algorithm for distributed anomaly detection by clustering \\n ellipsoids.  \\n3. Discuss the general architecture of a stream -processing system in IOT.  \\nCourse Outcome 4 (CO4):  \\n1. Give an overview on the security requirements of IOT.  \\n2. How can you nullify the impact of fault in  high-availability cluster?  \\n3. Explain the BCK with pre -shared keys for TinyTO.  \\n61 \\n Course Outcome 5 (CO5):  \\n1. Give an overview on the Wired Gateway Interfaces . \\n2. List the features to select the gateway hardware.  \\n3. List the steps to prepare Raspberry Pi for the execution.  \\n \\nModel Question Paper   \\nCourse Code: 20MCA281  \\n \\n Course Name: INTERNET OF THINGS  \\n                                                   \\nMax. Marks :60        Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1. What do you mean by computation offloading?  \\n2. Explain the framework that enables collaboration between smart mobile devices and \\ncloud.  \\n3. Outline the major challenges fac ed in the Fog paradigm.  \\n4. Explain Polyglot Programming.   \\n5. Which are the challenges faced by stream -processing systems?  \\n6. Explain anomaly detection and categorize anomalies in the data.  \\n7. List the different ways that an IOT gateway can extend connectivity to node s. \\n8. Write the advantages of obfuscation and diversification techniques.  \\n9. Explain Inter -Integrated Circuit (I2C) or Two Wire Interface (TWI).  \\n10. Write a short note on Zigbee.  \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n11. Explain the taxonomy of Resource Management in IOT.    (6 Marks)  \\nOR \\n12. Draw and explain the state diagram of the open IOT services life cycle.    (6 Marks)  \\n \\n13. a. Comment on the four broad requirements that motivate the design of TinyOS  \\n                                                                                             (3 Marks)  \\n       b. Describe the design decisions for nesC.        (3 Marks)  \\nOR \\n14. List t he features in coordination languages  - Linda, eLinda, Orc, and Jolie   \\n                     (6 Marks)  \\n \\n62 \\n 15. Compare Stream Management System (DSMS) and Complex Event Processing (CEP).  \\n(6 Marks)  \\nOR \\n16. Describe hyper ellipsoidal model for anomaly detection.          (6 Marks)  \\n \\n17. Describe the error detection techniques which are applicable in the context of an IOT.  \\n(6 Marks)  \\nOR \\n18. Explain the Station -to-Station protocol (STS) and the two main shortcomings of STS.  \\n(6 Marks)  \\n \\n19. Discuss the sensors required to build the environmental -sensing IoT gateway device for \\n weather monitoring.                 (6 Marks)  \\nOR \\n20. List and explain the six steps for the development of a sensor  project . (6 Marks)  \\n \\n \\n \\nSyllabus  \\nModule 1 (9 Hours)  \\n \\nOverview of Internet of Things:   Open -source semantic web infrastructure for managing \\nIOT resources in the Cloud - Device/Cloud Collaboration framework for intelligence \\napplications .  \\n \\nModule 2 (11 Hours)  \\n \\nIntroduction to Fog Computing:  principles, archi tectures, and applications. TinyOS – NesC, \\nProgramming frameworks for Internet of Things  \\n \\nModule 3 (8 Hours)  \\n \\nStream processing in IoT: foundations, state -of-the-art, and future directions - A framework \\nfor distributed data analysis for IoT  \\n \\nModule 4 ( 9 Hours)  \\n \\nSecurity and privacy in the Internet of Things - Internet of Things  - robustness and \\nreliability. TinyTO : two -way authentication for constrained devices in the Internet of \\nThings - Obfuscation and diversification for securing the Internet of Things  \\n63 \\n  \\nModule 5 (8 Hours)  \\n \\nCreating a simple IoT project - Preparing Raspberry Pi – Interfacing the hardware - Internal \\nrepresentation of sensor values - Persisting data - Creating the actuator project - Creating a \\ncontroller.  \\n \\nMore detailed knowledge may be acquired through seminars, assignments and talks by eminent \\nexternal experts and also by implementing a micro project.  \\n \\nAny one of the following or similar micro projects may be given as part of the course.  \\n1. Smart Gas Leakage Detector  \\n2. Night Patrol at home  \\n \\nText  Books  \\n1. RajkumarBuyya; Amir VahidDastjerdi ,  “Internet of Things”, Morgan Kaufmann,  2016  \\n \\nReference Books  \\n1. Peter Waher, “Learning Internet of Things”, Packt Publishing, 2015  \\n2. S. SitharamaIyengar; Nandan Parameswaran; Vir V. Phoha; N. Balakrishnan; Chuka Okoye, \\n“Fundamentals of Sensor Network Pr ogramming: Applications and Technology”, Wiley,  \\nDecember 14, 2010  \\n3. Robert Stackowiak, Art Licht, VenuMantha, Louis Nagode, “Big Data and The Internet of \\nThings: Enterprise Information Architecture for A New Age”, Apress, 2015  \\n \\nWeb Resources  \\n1. https://www.coursera.org/specializations/internet -of-things  \\n2. http://web.mit.edu/professional/digital -programs/courses/IoT  \\n \\nCourse Contents and Lecture Schedule  \\n \\nNo Topic  No. of \\nLectures  \\n1 Introduction  (9 Hours)  \\n1.1 Internet of things - definition, evolution. Applications -Smart home \\napplications, Health care, Elder care, Traffic surveillance.   \\n1 \\nSOA -Based Architecture, API oriented Architecture, Resource \\nManagement.  1 \\nComputational Offloading, Identification and Resource/Service \\nDiscovery, IOT Data Management and Analytics, IOT and the \\nCLOUD  1 \\n64 \\n 1.2 Open IOT architecture for IOT/Cloud convergence, Sensor \\nmiddleware, Cloud computing infrastructure, Directory service, \\nGlobal Scheduler, Local Scheduler component, Service delivery \\nand utility manager  1 \\nWorkflow of open IOT platform, Scheduling process and IOT \\nServices lifecycle, State diagram of the Open IOT Services lifecycle \\nwithin the scheduler module  1 \\nScheduling and resource management,  Resource optimization \\nschemes, Caching technique  1 \\nService creation flowchart, Comparison of cost - with cache server \\nand public cloud data -score  1 \\n1.3 Runtime adaptation engine,  Device/cloud collaboration framework  1 \\napplications of device/cloud collaboration, Semantic QA cache  1 \\n2 Programming frameworks  (11 Hours)   \\n2.1 Introduction to Fog Computing: principles, architectures, and  \\nApplications  1 \\nMotivating scenario for Fog Computing, Advantages of Fog \\nComputing, Reference architecture of Fog Computing  1 \\nSoftware -Defined Resource management layer, Services of \\nSoftware -Defined Resource management layer, Applications of \\nFog Computing.  1 \\n2.2 History of TinyOS , Implementation, Requirements motivating the \\ndesign of TinyOS, Component Model, Interfaces. TinyOS \\ncomputational concepts  1 \\n Overview of TinyOS Execution Model, Concurrency, TinyOS  \\nTheory of Execution: Events & Tasks, TinyOS Architecture. \\nTinyOS -Programming Model.  1 \\n2.3 nesC design, Component Implementation, Design Decisions for \\nnesC, Module Components, Configuration Components  1 \\nWhole -Program Analysis, Detecting Race Conditions, Dealing with \\nRace Conditions, Issues for nesC.  1 \\n2.4 Overview of  Embedded Programming Languages - nesC, Keil C, \\nDynamic C, B#, Message Passing in Devices -Remote Procedure \\nCall (RPC), Lightweight RPC (LRPC)  1 \\nRepresentational state transfer (REST), Computational REST \\n(CREST), Constrained Application Protocol(CoAP), Comparison \\nof HTTP and CoAP, Advantages of CoAP  1 \\n65 \\n Coordination Languages - Orchestration, Choreography, Linda and \\neLinda, Orc, Features of Orc, Java Orchestration Language \\nInterpreter Engine (Jolie), Polyglot Programming, Inverse pyramid \\nfor Polyglot Programming.  1 \\nFeatures of programming frameworks for IOT, IOT programming \\napproaches, Existing IOT frameworks  1 \\n3 Data management techniques  (8 Hours)  \\n3.1 Stream, Stream Processing, Data Stream Management System \\n(DSMS)  1 \\nComplex Event Processing (CEP), differences between two use -\\ncases of Stream Processing: DSMS and CEP  1 \\nThe characteristics of stream data in IOT, general architecture of a \\nstream -processing system in IOT  1 \\nContinuous logic processing system, challenges in stream -\\nprocessing systems.  1 \\n3.2 Anomaly detection, problem statement and definitions  1 \\nHyper ellipsoidal anomaly detection  1 \\nDistributed anomaly detection  1 \\nClustering ellipsoids, incremental local modeling  1 \\n4 Security and privacy  (9 Hours)  \\n4.1 IOT security threats, IOT security requirements, security \\nframeworks for IOT, IOT security overview, IOT gateways and \\nsecurity, IOT routing attacks  1 \\nSecurity frameworks for IOT - Lightweight cryptography, \\nasymmetric LWC algorithms, privacy in IOT networks  1 \\n4.2 IOT characteristics and reliability issues, reliability challenges  1 \\nAddressing reliability, security aspects and solutions  1 \\n4.3 TinyTO: Two -way authentication for constrained devices in the \\nInternet of Things  1 \\nTinyTO protocol  1 \\nBCK with pre -shared keys for TinyTO, handshake implementation  1 \\n4.4 IOT network stack and access protocols , Obfuscation and \\ndiversification techniques  1 \\n66 \\n Enhancing the security in IOT using obfuscation and diversification \\ntechniques, motivations and limitations, different use -case \\nscenarios on software diversification and obfuscation.  1 \\n5 IoT Implementation (8 Hours)   \\n5.1 Three key components to an IOT architecture, Sensor to gateway \\ncommunication - wired gateway interfaces, wireless gateway \\ninterfaces  1 \\nSensors - sensors required to build the environmental -sensing IOT \\ngateway device for weather monitoring  1 \\nGateway, Gateway hardware, Gateway software  1 \\nData transmission - advanced message queuing protocol, backend \\nprocessing, to CLOUD or not to cloud  1 \\n5.2 Creating a simple sensor project - Preparing Raspberry Pi – Clayster \\nlibraries  1 \\n \\nHardware, Interfacing the hardware - Internal representation of \\nsensor values - Persisting data  1 \\n \\nExternal representation of sensor values, Exporting sensor data  1 \\nCreating the actuator project – Hardware, Interfacing the hardware, \\nCreating a controller  1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n67 \\n 20MCA283  DEEP LEARNING  CATEGORY  L T P CREDIT  \\nELECTIVE  3 1 0 4 \\n  \\nPreamble: This course intends to provide insight into deep learning. This topic is currently a \\nmuch sought -after skill and is under active research.   Students have to refer appropriate \\nresearch papers and multiple books to get in -depth knowledge about  the topics . Instructors may \\ngive suitable programming assignments to augment the material covered in the classroom.   \\nPrerequisite: Basic concepts of linea r algebra, probability and optimization . \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s Category Level  \\nCO 1  Explain  the basic concepts of deep learning.  Level 2:  \\nUnderstand  \\nCO 2  Design neural networks using TensorFlow  Level 3:  \\nApply  \\nCO 3  Solve real world problems with CNN.  Level 3:  \\nApply  \\nCO 4  Solve real world problems with RNN.  Level 3:  \\nApply  \\nCO 5  Describe  the concepts of GAN.  Level 2:  \\nUnderstand  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 2           \\nCO 2  3 3 3  3  3      \\nCO 3  3 3 3  3  3      \\nCO 4  3 3 3  3  3      \\nCO 5  2 3   2  2      \\n3/2/1: High/Medium/Low  \\n \\nAssessment Pattern  \\nBloom’s \\nCategory  Continuous Assessment \\nTests  End Semester \\nExamination  1 2 \\nRemember  15 15 10 \\nUnderstand  25 25 30 \\nApply  10 10 20 \\nAnalyze     \\nEvaluate     \\nCreate     \\n68 \\n Mark distribution  \\nTotal \\nMarks  CIE ESE  ESE \\nDuration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can ha ve maximum 2 sub -divisions and carry 6 marks  \\n \\nCourse Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n1. Describe the model of a biological neuron . \\n2. Explain Perceptron learning algorithm . \\n3. Explain the role of batch normalization in training a neural network . \\nCourse Outcome 2 (CO2)  \\n1. Draw and demonstrate the VGG -16 architecture.  \\n2. Sketch the AlexNet architecture and explain its functionalities.  \\n \\n  Course Outcome 3(CO3):  \\n1. Design a convolutional neural network which can classify MNIST handwritten  data.  \\n2. An input image has been converted into a matrix of size 12 X 12 along with a filter of \\nsize 3 X 3 with a Stride of 1. Determine the size of the convoluted matrix.  \\n3. Why do we prefer Convolutional Neural networks (CNN) over Artificial Neural \\nnetworks (ANN) for i mage data as input?  \\n \\nCourse Outcome 4 (CO4):  \\n1. You are given an image data set with 10 classes . Describe how you will use deep \\nlearning to build a classifier.  \\n2. Design a system to generate deep fakes from an image.  \\n \\nCourse Outcome 5 (CO5):  \\n1. Describe auto encoders and how they help in dimensionality reduction.  \\n2. Explain how GANS work . \\n69 \\n Model Question Paper   \\nCourse Code: 20MCA283  \\n \\n Course Name: DEEP LEARNING  \\n                                                   \\nMax. Marks :60        Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n \\n1. Describe  sigmoid activation functions.  \\n2. Write the  gradient descent algorithm.  \\n3. Explain with an example how graphs are stored and represented in TensorFlow.  \\n4. Discuss how graph representation can accelerate computing models.  \\n5. Describe  the VGG 16 architecture.  \\n6. What is max pooling in the context of CNN?  \\n7. Explain ReLU.  \\n8. Explain the problem of vanishing gradients.  \\n9. Write a note on auto encoders.  \\n10. Explain the idea behind cross entropy.  \\nPart B  \\nAnswer one full question from each module, each carries 6 marks.  \\n11. (a) Describe the model of a biological neuron.     3 marks  \\n(b) Explain perceptron learning algorithm .      3 marks  \\nOR \\n12. With a suitable example explain how backpropagation  works     6marks  \\n \\n13. Explain the role of batch normalization in training a neural network and  \\ndescribe how to find out overfitting from training and validation curves    6 marks  \\nOR \\n14. Explain the ideas of Rank, Shape  and Type with an example in the  \\ncontext of a Tensor Data Stru cture       6 marks  \\n \\n15. With a suitable numerical example illustrate convolution operation.     6 marks  \\nOR \\n16. Explain the architecture of AlexNet.        6 marks  \\n70 \\n  \\n17. Explain the idea of Truncated backpropagation  through time.    6 marks  \\nOR \\n18. Describe how LSTM works .        6 marks  \\n \\n19. Distinguish between generative and discriminative models    6 marks  \\nOR \\n20. Explain how a GAN is trained .      6 marks  \\n \\n \\nSyllabus  \\nModule I (8 Hours)  \\nReview of Neural Networks: Model of a biological neuron, McCulloch Pitts Neuron, \\nActivation Functions, Perceptron, Perceptron Learning Algorithm and Convergence, \\nMultilayer Perceptron , Back propagation, Learning XOR , Sigmoid Neurons, Gradient \\nDescent, Feed forward Neural Networks . \\nModule II (10 Hours)  \\nTraining  Neural Networks: Initialization, dropout, batch normalization  and dropout, \\noverfitting, underfitting, training and validation curves . \\nData Visualization: Feature and weight visualization , tSNE.  \\nIntroduction to Tensor Flow: graphs, nodes, Tensor data structure s - rank, shape, type, \\nBuilding neural networks with Tensor Flow, Introduction to Keras . \\nModule III (10 Hours)  \\nConvolutional Neural Networks:  Convolution operation,  Convolutional layers in \\nneural network , pooling, fully connected layers . \\nCase study: Architecture of  Lenet, Alexnet and VGG 16  \\nModule IV (8 Hours)  \\nRecurrent Neural Networks:  Back propagatio n, vanishing gradients, exploding \\ngradients , truncated backpropagation through time,  Gated Recurrent Units (GRUs), \\nLong Short-Term Memory (LSTM) cells, solving the vanishing gradient problem \\nwith LSTMs . \\nModule V (9 Hours)  \\nAutoencoders, variational autoencoders . \\nGenerative Adversarial Networks (GAN):   Discriminative and generative models , \\nGAN discriminator, GAN g enerator, upsampling,  GAN Training , GAN challenges, \\nloss functions , cross entropy , minimax loss , Wasserstein loss . \\n        Programming assignments using TensorFlow maybe given at the end of each module  \\n        to get hands on experience.  \\n \\n71 \\n Textbooks.  \\n1. Generative Deep Learning: David Foster , OReily, (2019)  \\n2. Deep Learning , Ian Goodfellow, Yoshua Bengio and Aaron Courville, MIT press (2016)  \\n3. Hands on Machine Learning with Scikit Learn and TensorFlow, Aurélien Géron  (2019)  \\n4. Deep Learning Illustrated, Jon Krohn,  Grant Beyleveld,  Aglae Bassens, Pearson, 1st Edn.,  \\n(2020)  \\n5. Online book Dive Deep into Machine Learning at   https://d2l.ai/  \\nReferences  \\nModule 1  \\na. https://www.cse.iitm.ac.in/~miteshk/CS6910/Slides/Lecture2.pdf  \\nb. https://www.cse.iitm.ac.in/~miteshk/CS6910/Slides/Lecture3.pdf  \\nModule 2  \\na. http://neuralnetworksanddeeplearning.com  \\nb. Hands on Machine Learning with Scikit Learn and TensorFlow, Aurélien G éron \\nc. Probabilistic Machine Learning: An Introduction , Kevin Murphy  \\nd. https://www.researchgate.net/publication/228339739_Viualizing_data_using_t -SNE  \\nModule 3  \\na. https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture11.pdf  \\nb. Convolutional neural networks for visual computing  (Chapter 4 ), Ragav \\nVenkatesan and Baoxin Li CRC press  \\nModule 4  \\na. On the di fficulty of training RNNs : https://arxiv.org/pdf/1211.5063.pdf  \\nb. LSTM: A Search Space Odyssey : https://arxiv.org/abs/1503.04069  \\nc. Understanding Deriving and Extending the LSTM : https://r2rt.com/written -\\nmemories -understanding -deriving -and-extending -the-lstm.html  \\nd. Understanding LSTM Networks : http://colah.github.io/posts/2015 -08-\\nUnderstanding -LSTMs/  \\ne. https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture14.pdf  \\nf. https:/ /www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture15.pdf  \\nModule 5  \\na. GANs in Action:  Deep Learning with Generative Adversarial Network Jakub  \\nLanggr , Vladimir Bok    \\nb. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and \\nPlay David Foster  \\nc. https://developers.google.com/machine -learning/gan  \\n \\n \\n \\n \\n \\n72 \\n Course Contents and Lecture Schedule  \\n \\nNo Topic  No. of \\nLectures  \\n1 Module 1  8 Hours  \\n1.1 Review of Neural Networks : Model of a biological neuron  1 \\n1.2 McCulloch Pitts Neuron, Activation functions  1 \\n1.3 Perceptron, Perceptron Learning Algorithm  1 \\n1.4 Convergence, Multilayer Perceptron  1 \\n1.5 Back propagation  1 \\n1.6 Learning XOR , Sigmoid Neurons  1 \\n1.7 Gradient Descent, Feed forward Neural Networks  2 \\n2 Module 2 10 Hours  \\n2.1 Training Neural Networks  1 \\n2.2 Initialization, Dropout  1 \\n2.3 Batch normalization  and drop out  1 \\n2.4 Over fitting, under fitting, training and validation curves, data \\nvisualization, feature and weight visualization, tSNE  2 \\n2.5 Introduction to  Tensor Flow, graphs, nodes, Tensor Data \\nStructure s - rank, shape, type  2 \\n2.6 Building neural networks with tensor flow  2 \\n2.7 Introduction to Keras  1 \\n3 Module 3 10 Hours  \\n3.1 Convolutional neural networks  1 \\n3.2 Convolution operation  2 \\n3.3 Back propagation in multilayer neural networks  1 \\n3.4 Convolutional layers in neural network , pooling  2 \\n3.5 Fully connected layers  2 \\n3.6 Case study: Architecture of  Lenet, Alexnet  and VGG 16  2 \\n4 Module 4 8 Hours  \\n4.1 Recurrent neural networks  1 \\n4.2 Back propagation: vanishing gradients, exploding gradients  1 \\n4.3 Truncated Backpropagation Through Time  1 \\n4.4 LSTM  1 \\n4.5 Gated Recurrent Units (GRUs)  1 \\n4.6 Long Short -Term  Memory (LSTM) Cells  1 \\n4.7 Solving the vanishing gradient problem with LSTMs  2 \\n5 Module 5 9 Hours  \\n5.1 Autoencoders, Variational autoencoders  2 \\n5.2 Generative Adversarial Networks  (GAN)  1 \\n5.3 Discriminative and generative models  2 \\n5.4 GAN Discriminator , GAN  Generator, upsampling,   1 \\n5.5 GAN Training  1 \\n5.6 GAN challenges , Loss functions , cross entropy , minimax loss, \\nWasserstein loss  2 \\n \\n73 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA285  DIGITAL IMAGE \\nPROCESSING  ELECTIVE  3 1 0 4 \\n \\nPreamble: This course introduces the techniques of simulating human vision into computer \\nvision based on feature extraction to develop applications in different areas. The concept of \\nenhancement, transforms, smoothing, restoration, compression, morphologic al image analysis, \\nclassification & segmentation in two -dimensional space are introduced. This course serves as \\na prerequisite for many advanced courses in computer vision areas.   \\nPrerequisite: Nil \\nCourse Outcomes:   After completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Discuss  the fundamental concepts of digital image \\nprocessing, image formation and representation of images.  Level 2: \\nUnderstand  \\nCO 2  Summarise  image enhancement methods in the spatial \\ndomain.  Level 2: \\nUnderstand  \\nCO 3  Explain  image transforms  and image smoothing & \\nsharpening using various kinds of filters in frequency \\ndomain . Level 2: \\nUnderstand  \\nCO 4  Describe various methods in image restoration  and \\ncompression.  Level 2: \\nUnderstand  \\nCO 5  Discuss  morphological basics and image segmentation \\nmethods.  Level 2: \\nUnderstand  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 2     2      \\nCO 2  2 2     2      \\nCO 3  2 2     2      \\nCO 4  2 2     2      \\nCO 5  2 2     2      \\n  3/2/1: High/Medium/Low  \\n \\n \\n \\n \\n74 \\n Assessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2  \\nLevel 1: Remember  15 15 20 \\nLevel 2: Understand  35 35 40 \\nLevel 3: Apply     \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can ha ve a maximum of 2 sub -divisions and carry 6 marks.  \\nSample Course Level Assessment Questions  \\nCourse Outcome 1 (CO 1):  \\n1. List out various components of an Image Processing System.  \\n2. Define Electromagnetic Spectrum.  \\n3. Illustrate the image formation in the eye. Calculate the size of the retinal image of a \\ntree, if the observer is looking at a tree 20 m high at a distance of 100.  \\nCourse Outcome 2 (CO 2):  \\n4. Describe the  basic relationships and distance measures between pixels in a digital \\nimage.  \\n5. List and explain steps in Histogram Processing.  \\n6. List and explain various Intensity transformation functions  used in grey scale images.  \\n \\n75 \\n 7. Explain the process of Unsharp masking?  \\nCourse Outcome 3 (CO 3):  \\n1. Explain the properties of Unitary transform.  \\n2. Compare and contrast 1D-DFT  and 2D-DFT . \\n3. Design a basic Laplacian filter using first order and second order derivatives . \\n4. Describe  various image smoothing techniques using frequency domain filters . \\nCourse Outcome 4 (CO 4):  \\n1. Explain  image noise models  and list out different noise probability density functions \\nused in image processing applications.  \\n2. Describe Wiener filtering tec hnique.   \\n3. Draw the functional block diagram of image compression system. List various types of \\nredundancy in compression ?  \\nCourse Outcome 5 (CO 5):  \\n1. Differentiate erosion and dilation in morphological processing . \\n2. Compare Global thresholding and Otsu’s method . \\n3. Explain how does Hough transform works.  \\n \\n      Model Question Paper   \\n \\nCourse Code: 20MCA285  \\n \\n Course Name: Digital Image Processing  \\n \\nMax.  Marks :60        Duration: 3 Hrs  \\nPart A  \\nAnswer all questions.  \\nEach question carries 3 marks (10 x 3 = 30 Marks)  \\n.  \\n1. Describe the elements of visual perception.  \\n2. Define Toeplitz & Circulant matrices  \\n3. Explain histogram equalization in detail.  \\n4. Differentiate linear spatial filter and non -linear spatial filter.  \\n5. Explain the properties of 2D DFT . \\n6. List the steps involved in frequency domain filtering . \\n7. Write note on Point Spread Function.  \\n8. List the components of a compression system.  \\n9. Compare opening a nd closing in morphological processing of images . \\n10. Explain the merits and demerits  of edge thresholding in segmentation .                                                                                             \\n                                          \\n (10 x 3=30 marks)  \\n \\n76 \\n                                 Part B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 =  30 Marks)  \\n \\n11. Explain fundamental steps in Digital Image Processing . \\n                                                                                           (6) \\n       OR \\n \\n12. Differentiate sampling and quantization in image processing.       (6) \\n \\n \\n13. Explain bas ic grey level transformation in spatial domain.     (6) \\n \\n                                       OR \\n14. Compare Unsharp masking and High -boost filtering  in Spatial filtering   (6) \\n \\n15. Explain D iscrete Cosine Transform  and its properties .                (6) \\n \\n OR \\n16. Explain the working of Homomorphic filtering with an example.     (6) \\n \\n17. Explain image restoration process in detail.        (6) \\n \\n  OR  \\n18.  Differentiate lossy and lossless image compression methods    (6) \\n \\n19. Compare erosion and dilation in Morphological image analysis    (6) \\n                        OR \\n20. Explain canny edge detector in detail        (6) \\n   \\n                                                                                                              (5 x 6=30 Marks)  \\n \\n \\n \\n \\n \\n \\n \\n \\n77 \\n Syllabus  \\nModule 1: Overview of Digital Image Processing (9 Hours)  \\nDigital Image Processing: Basic concepts, Difference between image processing and \\ncomputer vision, Components of an image processing system. Image processing applications.  \\nMathematical preliminaries: Basic Vector and Matrix operations, Toeplitz, Circulant, \\nUnitary & Orthogonal matrices.  \\nElements of Visual Perception: Structure of the human eye and image formation, Brightness \\nadaptation and discrimination.  Types of Images: Binary, Gray scale and Color Images. Image \\nSampling and Quantization: Digital image as a 2D array, Spatial and Intensity resol ution, \\n2D-sampling theorem. RGB and HSI color models.  \\n \\nModule 2: Concept of Image enhancement  & Spatial filtering  (10 Hours)  \\nConcept of Image enhancement, Basic grey level transformation functions: Image negative, \\nLog transformation, Power -law transformation, Piecewise linear transformations. Histogram \\nof an Image, Histogram equalization with illustration . \\nFundamentals of Spatial Filtering: Mechanics of Spatial filtering, 2D correlation and \\nconvolution.  \\nSmoothing spatial filters: Linear and Non linear types.   \\nSharpening spatial filters: Laplacian operator, Unsharp masking and High -boost filtering, \\nGradient based operators for image sharpening.  \\n \\nModule 3: Image Transform  & Filtering in frequency domain (8 Hours)  \\nImage Transform -representation of an image in frequency  domain, Unitary transformation of \\nan Image -transform pair equations in matrix form, Properties of unitary transforms. 1D -DFT, \\n2D-DFT of an image - Properties  of 2D -DFT. DCT and its properties, Filtering an Image in \\nthe Frequency Domain – Steps of frequency domain filtering. Basic concept and illustration \\nof frequency domain image smoothing and sharpening.  \\n \\nModule 4:  Image Restoration & Compression  (8 Hours)  \\nImage Restoration: Concept of Image restoration, A Model of the Image \\nDegradation/Restoration Process, Image Noise Models, Point Spread Function, Restoration \\nusing Inverse filtering, Wiener filtering.  \\nImage compression: Need for compression, redundancy, classification of image compression \\nschemes, A general image compression system, Huffman coding,  Transform based \\ncompression, JPEG standard, Digital image watermarking -basic concept.  \\n \\nModule 5: Basics of m orphological image proce ssing  & image s egmentation  (10 \\nHours)  \\nMorphological image processing  basics : erosion and dilation, opening and closing, Hit or \\nMiss transformation.  \\n78 \\n Image segmentation: Fundamentals, Point detection, Line detection, Basic steps of edge \\ndetection, Hough transform , Edge detectors - Marr -Hildreth edge detector  & Canny edge \\ndetecto r. Thresholding: Basics of intensity thresholding, Global thresholding and Otsu’s \\nmethod. Region -based segmentation: Region growing, Region Splitting and Merging.  \\n \\n \\n \\nText Books  \\n1. Rafael C., Gonzalez & Woods R.E., “Digital Image Processing”, Pearson Education.  \\n2. Jain A.K, “Fundamentals of Digital Image Processing”, Prentice Hall, Eaglewood Cliffs, \\nNJ.  \\nReference Books  \\n1.Schalkoff R. J., “Digital Image Processing and Computer Vision”, John Wiley 5. Pratt \\nW.K., “Digital Image Processing”, John Wiley  \\n2.Al Bovick , \"Handbook of Image and Video Processing\" , Academic Press,   2000  \\n \\nCourse Contents and Lecture Schedule  \\nSl. \\nNo. Topic  No. of \\nLectures  \\n1 Overview of Data Image Processing  9 Hours  \\n1.1 Basic concepts  of d igital image processing , Image processing \\napplication  2 \\n \\n1.2 Mathematical preliminaries: Basic Vector and Matrix operations  1 \\n1.3 Toeplitz, Circulant, Unitary & Orthogonal matrices  1 \\n1.4 Elements of Visual Perception , Structure of human eye  1 \\n1.5 Brightness adaptation and discrimination , Types of Images  1 \\n1.6 Sampling and Quantization  1 \\n1.7 Spatial and Intensity resolution, 2D -sampling theorem.  1 \\n1.8 RGB and HSI color models.  1 \\n2 Concept of Image enhancement  & Spatial filtering  10 Hours  \\n2.1 Concept s of Image enhancement, Basic grey level transformation \\nfunctions: Image negative, Log transformation, Power -law \\ntransformation, Piecewise linear transformations.  2 \\n2.2 Histogram of an Image, Histogram equalization with illustration  1 \\n79 \\n 2.3 Fundamentals of Spatial Filtering: Mechanics of Spatial filtering  1 \\n2.4 2D correlation and convolution  1 \\n2.5 Smoothing spatial filters: Linear and Nonlinear types  1 \\n2.6 Sharpening spatial filters: Laplacian operator  1 \\n2.7 Unsharp masking  1 \\n2.8 High -boost filtering  1 \\n2.9 Gradient based operators for image sharpening  1 \\n3 Image Transform  & Filtering in frequency domain  8 Hours  \\n3.1 Image Transform -representation of an image in the transform domain.  1 \\n3.2 Unitary transformation of an Image , Properties of unitary transforms  1 \\n3.3 1D-DFT  1 \\n3.4 2D-DFT and its properties  1 \\n3.5 DCT and its properties  1 \\n3.6 Filtering an Image in the Frequency Domain – Steps of frequency \\ndomain filtering  1 \\n3.7 Image smoothing  using frequency domain filters – Ideal Lowpass \\nFilters, Butterworth Lowpass filter & Gaussian Lowpass filter.   1 \\n3.8 Image s harpen ing using frequency domain filters – Ideal Highpass \\nFilters, Butterworth Highpass filter & Gaussian Highpass filter.   1 \\n4 Image Restoration & Image compression  8 Hours  \\n4.1 Image Restoration: Concept of Image restoration  1 \\n4.2 A Model of the Image Degradation/Restoration Process  1 \\n4.3 Image Noise Models  1 \\n4.4 Point Spread Function  1 \\n4.5 Restoration using Inverse filtering, Wiener filtering  1 \\n4.6 Image compression: Need for compression, redundancy, classification \\nof image compression schemes  1 \\n4.7 A general image compression system Huffman coding, Transform \\nbased compression.  1 \\n4.8 JPEG standard, Digital image watermarking -basic concept.  1 \\n5 Basics of m orphological image processing  & image s egmentation   10 Hours  \\n5.1 Morphological image processing  basics : erosion and dilation, opening \\nand closing, Hit or Miss transformation.  1 \\n5.2 Image segmentation: Fundamentals, Point detection, Line detection  1 \\n5.3 Basic steps of edge detection  - Hough transform  2 \\n5.4 Edge detectors: Marr -Hildreth edge detector  1 \\n80 \\n 5.5 Canny edge detector  1 \\n5.6 Thresholding: Basics of intensity thresholding.  1 \\n5.7 Global thresholding  1 \\n5.8 Otsu’s method  1 \\n5.9 Region -based segmentation: Region growing, Region Splitting and \\nMerging.  1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n81 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\nS20MCA287  BIOINFORMATICS  ELECTIVE  3 1 0 4 \\n \\nPreamble: This course helps to understand the concepts of computational biology and \\nbioinformatics. The students will learn Database tools and their uses, various algorithms for \\nbiological sequence analysis, Genomics and Gene Recognition, Protein structure and to use \\nvarious visualization techniques, data mining & machine learning in bioinformatics.  \\nPrerequisite:  Nil \\nCourse Outcomes:   After completion of the course  the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Explain the fundamentals of Computational Biology and \\nBioinformatics.  Level 2: \\nUnderstand  \\nCO 2  Classify various biological databases.  Level 2: \\nUnderstand  \\nCO 3  Use suitable algorithm for Biological Sequence Analysis and \\nmake use of database search tools.  Level 3:  \\nApply  \\nCO 4  Discuss Gene structure and expression of Prokaryotic and \\nEukaryotes.  Level 2: \\nUnderstand  \\nCO 5  Apply data mining & machine learning methods to analyse and \\nvisualize biological data.  Level 3:  \\nApply  \\nMapping of Course Outcomes with Program Outcomes  \\n            PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 1 1    1      \\nCO 2  3 3 1    2      \\nCO 3  3 3 2    2      \\nCO 4  3 2 1    1      \\nCO 5  3 3 2  2  2      \\n  3/2/1: High/Medium/Low  \\n \\n \\n82 \\n Assessment Pattern  \\nBloom’s Category \\nLevels  Continuous \\nAssessment \\nTests  End \\nSemester \\nExamination  \\n1 2  \\nLevel 1: Remember  10 10 10 \\nLevel 2: Understand  30 30 30 \\nLevel 3: Apply  10 10 20 \\nLevel 4: Analyse     \\nLevel 5: Evaluate     \\nLevel 6: Create     \\nMark distribution  \\nTotal \\nMarks  Continuous Internal \\nEvaluation (CIE)  End Semester \\nExamination \\n(ESE)  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance       : 8 marks  \\nContinuous Assessment Test (2 numbers)   : 20 marks  \\nAssignment/Quiz/Course project    : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A  and Part B . Part A \\ncontain 10 questions with 2 questions from each module, having 3 marks for each question. \\nStudents should answer all questions. Part B contains 2 questions from each module of which \\nstudent should answer any one . Each question can have a maximum 2  subdivisions and carry \\n6 marks.  \\n Sample Course Level Assessment Questions  \\n Course Outcome 1 (CO 1):  \\n1. Explain the concept of DNA  \\n2. Explain the concept of RNA . \\n3. Illustrate the concept of translation and transcription . \\n4. Discuss Gnome project and its impact on bioinformatics  \\n83 \\n Course Outcome 2 (CO 2):  \\n1. Explain the features of biological databases?  \\n2. Discuss primary sequence databases and secondary sequence databases.  \\n3. Classify the two important classification schemes of structu re classification databases.  \\n4. Retrieve the sequence from primary / secondary databases.  \\n5. Use of BLAST for comparing sequences . \\nCourse Outcome 3 (CO 3):  \\n1. Explain the importance of scoring matrices in sequence alignment.  \\n2. Explain the  different algorithms used f or sequence alignment .  \\n3. Illustrate Local and global alignment Algorithm for the sequence CGTGAATTCAT \\n(sequence#1 or A) GACTTAC (sequence #2 or B)  \\n4. Compute the best alignment of these two sequences: ACTGATTCA ACGCATCA \\nUsing -2 as a gap penalty, -3 as a mismatch penalty, and 2 as the score for a match.  \\nCourse Outcome 4 (CO 4):  \\n1. Explain the Prokaryotic gene structure  \\n2. Explain the Eukaryotic gene structure  \\n3. Demonstrate  the usage of Open Reading Frame with an example  \\na. 5′-ATCTAAAATGGGTGCC -3′  \\n4. Explain the workin g principle of microarray  \\nCourse Outcome 5 (CO 5):  \\n1. Differentiate between the different protein molecular structure visualizations.  \\n2. Use Web -based Map Viewer program, RasMol, PyMol   data visualization techniques \\nin bioinformatics .    \\n3. Use PubMed to search for a particular pattern to specify the importance of mining the \\nbiomedical literature for data on functions to complement the sequence and structure \\ndata mined from nucleotide and protein databases.  \\n4. Compare any three machine learning technologies and thei r applicability to data \\nmining methods .    \\nModel Question Paper   \\nCourse Code: S20MCA287  \\n Course Name: BIOINFORMATICS  \\nMax.Marks :60                 Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks  (10 * 3 = 30 Marks)  \\n1. Write short note on “how genome carries hereditary data from organisms.”  \\n2. What is antisense RNA?  \\n3. Write a short note on primary database.  \\n84 \\n 4. Write note o n : (a) SCOP    (b) CATH.  \\n5. How many times faster is to find the best alignment for the sequences \\n“RQQEPURSTC”  \\nand “QQESGPVRST” using N_W algorithm compared to assessing each possible \\nalignment one by one?  \\n6. Define raw score, bit score and e -value in BLAST.  \\n7. Write a short note on of process is gene expression in Prokaryotic.  \\n8. Justify the reasons for the high Prokaryotic gene density compared to Eukaryotes.  \\n9. Differentiate between centralized and distributed data mining Infrastructure. Use \\ndiagrams if necessary.  \\n10. Explain the significance of Hidden Markov Model in bioinformatics. Draw a sample \\nMarkov chain which is the basis of HMM, also mention how HMM is different from \\nMarkov chain.  \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n \\n11 What is the Central dogma of Molecular biology? How can Molecular biology be \\nconsidered as an information science?  6 \\n  OR   \\n12 With a neat diagram describe the structural and functional differences between \\nDNA and RNA?  6 \\n     \\n13 Explain different types of protein databases and its applications in bioinformatics.  6 \\n    OR   \\n14 Differentiate between Composite protein sequence database and secondary \\ndatabases .  6 \\n15 Align the following sequence using Needleman and Wunch algorithm for global \\nalignment ATTGC and AGGC with match +1, mismatch -1 and gap penalty -2. \\nWhat is the score of the optimal global alignment?  6 \\n    OR   \\n16 Find the best local alignment between ACCTAGG and GGCTCAATCA with +2 \\nfor a match, -1 for a mismatch and -2 for a gap using Smith Waterman Algorithm.  \\nExplain Prokaryotic Gene structure with neat diagram.  6 \\n17 What is GC content? How it differs in eukaryotic and prokaryotic genomes.           6 \\n    OR   \\n18 Describe with the help of a diagram the generation of cDNAs. Mention its use and \\nalso write notes on ESTs.  6 \\n85 \\n 19 Illustrate with the help of a neat diagram the pattern recognition and the label \\ndiscovery process.  6 \\n    OR   \\n20 Justify the importance of user interfaces in data visualization. With the support of \\na representative block diagram explain the structure of a 3D protein visualization \\ntool. Also explain the UI components of the same.  6 \\nSyllabus  \\nModule 1: Computational Biology and Bioinformatics   (7 Hours)  \\nComputational Biology: Cell - Central Dogma of Molecular Biology - Structure of DNA, \\nRNA and Protein - Coding and Non -coding RNAs - mRNA, tRNA, miRNA and siRNA.  \\nBioinformatics: Nature & Scope of Bioinformatics, Gnome projects, Importance of \\nbioinformatics , Pattern recognition and prediction.   \\nModule 2: Biological Databases (8 Hours)  \\nBiological Databases, Primary Sequence Databases, Composite protein sequence databases, \\nSecondary Databases, Composite protein pattern databases, Structure classification \\ndatabases.  \\n//Tutorial class may be arranged to the introduction and use of sequence retrieval from the \\ndatabases.  \\nModule 3: Data Searches and pairwise Alignment (10 Hours)  \\nDot Plots, Concept of Simple  Alignment, Scoring matrices: Introduc tion to  PAM & Blosum, \\nNeedleman and Wunsch Algorithm, Global and Local Alignments, Smith Waterman \\nAlgorithm, Multiple Sequence Alignment.  Familiarize Database search tools: BLAST & \\nFastA  \\n//Tutorial class may be arranged to the introduction and use  of se quence alignment and \\nBLAST.  \\nModule 4: Genomics and Gene recognition (10 Hours)  \\nIntroduction to Gene expression in prokaryotes, Prokaryotic Gene structure, GC content in \\nprokaryotic genomes, Gene Density. Eukaryotic Genomes: Gene structure, GC content in \\neukaryotic genomes, Gene Expression – Introduction to Microarrays.  \\nModule 5: Data Visualization, Data mining and Machine learning (10 Hours)  \\nData Visualization - Introduction, Sequence Visualization, Structure Visualization, User \\nInterface, Animation Versus Simulation, General -Purpose Technologies.       \\n \\n86 \\n Data Mining using biological data, Methods, Infrastructure, Pattern recognition and \\ndiscovery, Genetic Algorit hms, Neural networks using biological data, Statistical methods \\nusing biological data, Introduction to Hidden Markov Models and Text mining.  \\n//Tutorial class may be arranged to introduc e and use  - RasMol and PyMol .   \\nText Books  \\n1. Dan. E. Krane and M. L. Raymer, “Fundamental Concepts of Bioinformatics”, \\nPearson Education, 2003(Module 4)  \\n2.  Bryan Bergeron, M.D, “Bioinformatics Computing”, Pearson Education, 2015.  \\n(Module 1,5)  \\n3. Attwood T. K. and D. J. Parry -Smith,”  Introduction  to Bioinformatics”, Pearson \\nEducation, 2003 (Module 2,3)  \\n4. Neil C Jones and Pavel A Pevzner, “An Introduction to Bioinformatics Algorithms”, \\nMIT Press, 2004  \\nReference Books  \\n1. Jean-Michel Claverie and  Cedric Notredame, “Bioinformatics For Dummies” , 2nd \\nEdition,Wiley Publishing  \\n2. David W Mount, “Bioinformatics - Sequence and Genome Analysis “ , 2/e, Cold \\nSpring Harbor  \\n3. Laboratory Press, New York.  \\n4. “Bioinformatics for Dummies”  J. Claverie & C. Notredame ,Wiley India..  \\nWeb Reference  \\n1. https://nptel.ac.in/courses/102/106/102106065/  \\nCourse Contents and Lecture Schedule  \\nNo Topic  No. of \\nLectures  \\n1 Computational Biology and Bioinformatics    (7 Hours)  \\n1.1 Cell - Central Dogma of Molecular Biology, Structure of DNA  1 \\n1.2 RNA and Protein: Coding and Non -coding RNAs -mRNA  1 \\n1.3 tRNA, miRNA and siRNA  1 \\n1.4 Nature & Scope of Bioinformatics, Gnome projects  1 \\n1.5 Importance of bioinformatics, Pattern recognition and prediction  1 \\n1.6 Folding problem  1 \\n1.7 Sequence analysis, homology and analogy  1 \\n87 \\n 2 Biological Databases     (8 Hours)  \\n2.1 Primary Sequence databases: Nucleic acid and Protein sequence: PIR, \\nMIPS, SWIS -PROT  1 \\n2.2 Protein sequence: TrEMBL, NRL -3D 1 \\n2.3 Composite protein sequence Databases: NRDB, OWL, MIPSX and \\nSWISS -PROT+TrEMBL  1 \\n2.4 Secondary Databases,  Need for Secondary databases  1 \\n2.5 Prosite  1 \\n2.6 Prints  1 \\n2.7 Blocks, Profile, Pfam, Identify  1 \\n2.8 Composite Protein Pattern Database and Structure Classification \\nDatabases  1 \\n3 Data Searches and pairwise Alignment  (10 Hours)  \\n3.1 Dot Plots  1 \\n3.2 Concept of  Simple Alignment, GAPS  1 \\n3.3 Scoring matrices  1 \\n3.4 Introduction to PAM  1 \\n3.5 Introduction to Blosum  1 \\n3.6 Needleman and Wunsch Algorithm  1 \\n3.7 Global and Local Alignments: Semiglobal alignment  1 \\n3.8 Smith Waterman Algorithm  1 \\n3.9 Alignment scores and statistical significance of database search , Multiple \\nSequence Alignment.  1 \\n3.10 Familiarize Database search tools: BLAST & FastA  1 \\n4 Gene structure and expression of Prokaryotic and Eukaryotes.  (10 Hours)  \\n4.1 Introduction to Gene expression in Prokaryotes  1 \\n4.2 Prokaryotic Gene structure  1 \\n4.3 GC content in prokaryotic genomes  1 \\n4.4 Prokaryotic     Genomes  -Gene Density  1 \\n4.5 Eukaryotic  Genomes  1 \\n4.6 Gene structure ,  ORF in Prokaryotic  1 \\n88 \\n 4.7 GC   content in Eukaryotic  Genomes  1 \\n4.8 Gene Expression - cDNAs & ESTs,  1 \\n4.9 Serial Analysis of Gene Expression  1 \\n4.10 Introduction to Microarrays.  1 \\n5 Data Visualization, Data mining & Machine learning  (10 Hours)  \\n5.1 Data Visualization Introduction  1 \\n5.2 Sequence Visualization - Sequence Map  1 \\n5.3 Structure Visualization - Rendering tools  1 \\n5.4 User Interface - User Interface Components, Alternative Metaphors, \\nDisplay Architecture  1 \\n5.5 Animation Versus Simulation, General -Purpose Technologies.       1 \\n5.6 Data Mining, Methods, Infrastructure  1 \\n5.7 Pattern recognition and discovery  1 \\n5.8 Genetic Algorithms  1 \\n5.9 Neural networks, Statistical methods  1 \\n5.10 Hidden Markov Models and Text mining  1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n89 \\n 20MCA289  SOCIAL NETWORK \\nANALYSIS  CATEGORY  L T P CREDIT  \\nELECTIVE  3   1 0 4 \\n  \\nPreamble: This course intends to provide insight into social network analysis. The objective \\nof this course is to  enable students analyse  and visualize  network data . This course will create \\nan understanding about the semantic web, structure of various social networks and the structure \\nof search engines.   \\n \\nPrerequisite: Basic concepts of graph theory and networks  \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Explain the basic concepts of semantic web and social network \\nanalysis.  Level 2:  \\nUnderstand  \\nCO 2  Describe the ontolog y-based knowledge representation \\ntechniques in social network.    Level 2:  \\nUnderstand  \\nCO 3  Discuss  aggregat ion of social network information and \\nrepresent ation of  social individuals and social relationships.  Level 2:  \\nUnderstand  \\nCO 4  Describe the structure of the Web and Facebook as a graph and \\nthe algorithms for searching and community discovery.  Level 2:  \\nUnderstand  \\nCO 5  Explain the general architecture of a search engine and \\nspecifically the Google search engine architecture.  Level 2:  \\nUnderstand  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 2     1      \\nCO 2  2 2     1      \\nCO 3  2 2     2      \\nCO 4  2 3  2 2 2 2   2   \\nCO 5  2 3  2 2  2      \\n3/2/1: High/Medium/Low  \\n \\n \\n \\nAssessment Pattern  \\nBloom’s \\nCategory  Continuous Assessment \\nTests  End Semester \\nExamination  1 2 \\nRemember  15 15 20 \\n90 \\n Understand  35 35 40 \\nApply     \\nAnalyze     \\nEvaluate     \\nCreate     \\n \\n \\nMark distribution  \\nTotal Marks  CIE ESE  ESE Duration  \\n100 40 60 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nAttendance      : 8 marks  \\nContinuous Assessment Test (2 numbers)  : 20 marks  \\nAssignment/Quiz/Course project   : 12 marks  \\n \\nEnd Semester Examination Pattern:  There will be two parts; Part A and Part B. Part A \\ncontains 10 compulsory short answer questions, 2 from each module. Each question carries 3 \\nmarks.  Part B contains 2 questions from each module of which student should answer any one. \\nEach question can ha ve maximum 2 sub -divisions and carry 6 marks . \\n \\nCourse Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n1. Explain the development of semantic Web and the emergence of Social Web.  \\n2. Describe  the global structure social networks.  \\n3. Discuss in detail about the macro -structure of social networks.   \\n4. “Most network analysis methods work on an abstract, graph -based representation of \\nreal-world networks ”. Justify this statement.  \\n \\nCourse Outcome 2 (CO2)  \\n1. Describe the characteristics of Resource Description Framework (RDF).  \\n2. Compare the features of Web Ontology Language (WOL) and Unified Modeling \\nLanguage (UML).  \\n3. Compare the features of Web Ontology Language (WOL) and Entity Relationship (ER) \\nModel.  \\n \\nCourse Outcome 3(CO3):  \\n1. Describe the ontological representation of social individuals.  \\n2. Explain the generic architecture of Semantic Web applications.  \\n3. Discuss how semantic web applications can be built with social network features?  \\n91 \\n  \\nCourse Outcome 4 (CO4):  \\n1. Describe Zipf’s Law.  \\n2. Write the limitations of HyperANF Algorithm and explain how it can be sorted out \\nusing the Iterative Fringe Upper Bound (iFUB) Algorithm.  \\n3. What is meant by Degree Assortativity? What is the use of this measure?  \\n4. “A user who logs in more generally has more friends on Facebook”, describe how can \\nwe conclude this statement.  \\n \\nCourse Outcome 5 (CO5):  \\n1. Draw the architecture of a general search engine and explain how it works.  \\n2. Explain how the HITS Algorithm works to assi gn ranks to web pages.  \\n3. Compare the HITS Algorithm and the Page Rank Algorithm.  \\nModel Question Paper   \\nCourse Code: 20MCA289  \\n \\n Course Name: SOCIAL NETWORK ANALYSIS  \\n                                                   \\nMax. Marks :60        Duration: 3 Hrs  \\nPart A  \\n                   Answer all questions. Each question carries 3 marks (10 * 3 = 30 Marks)  \\n1. What is meant by semantic web?  \\n2. Write notes on personal networks.  \\n3. Define Electronic discussion networks.  \\n4. List out the features of blogs that can be used for social network extraction . \\n5. Explain how the reasoning with instance equality is done in social network data?  \\n6. What is meant by Evaluating Smushing?  \\n7. Define “Power Law”.  \\n8. What is “Spid”? How it is used to differentiate between web -network and social \\nnetwork?  \\n9. What are the basic functions of the storage repository of a search engine?  \\n10. How can we identify web spam pages?  \\nPart B  \\nAnswer all questions. Each question carries 6 marks. (5 * 6 = 30 Marks)  \\n11. List and explain various  measures in network analysis.    6 marks  \\n92 \\n OR \\n12. Describe the macro -structure of social networks.     6 marks  \\n \\n13. What is meant by ontology -based knowledge representation? Explain its  \\nrole in the semantic web.        6 marks  \\nOR \\n14. Compare the features of Web Ontology Language (WOL) and Extensible  \\nMarkup Language (XML).         6 marks  \\n \\n15. Describe how aggregating and reasoning can be done on social network data . \\n         6 marks  \\nOR \\n16. Discuss  the ontological representation of social relationships .   6 marks  \\n \\n17. Define the following with suitable example:  \\na) Rank exponent         2 marks  \\nb) Hop plot exponent         2 marks  \\nc) Eigen exponent         2 marks  \\n \\nOR \\n18. Explain how to generate in -degree and out -degree distributions on the  \\ngraph of the Web crawl.         6 marks  \\n \\n19. Describe how the web crawler module in a search engine does the page  \\nselection and page ref resh.        6 marks  \\nOR \\n20. Draw the architecture of Google search engine and comment on each  \\nof its components.         6 marks  \\n \\nSyllabus  \\nModule I (9 Hours)  \\nIntroduction to the Semantic Web and Social Networks:  \\nThe Semantic Web, Limitations of the current Web, The semantic solution, \\nDevelopment of the Semantic Web, The emergence of the social web, Social Network \\nAnalysis , Development of Social Network Analysis, The global structure of networks, \\nThe macro -structure of social networks, Personal networks.  \\nModule II (8 Hours)  \\nElectronic sources for network analysis: Electronic discussion networks, Blogs and \\nonline communities, Web -based networks.  \\n93 \\n Knowledge Representation on the Semantic Web: Ontologies and their role in the \\nSemantic Web, Ontology languages for the Semantic Web,  The Resource Description \\nFramework (RDF) and RDF Schema, The Web Ontology Language (OWL), \\nComparison of Ontology languages with the Unified Modelling Language (UML), \\nComparison to the Entity/Relationship (E/R) model and the Relational model, \\nComparison to  the Extensible Markup Language (XML) and XML Schema.  \\nModule III (8 Hours)  \\nModelling and aggregating social network data : \\nNetwork data representation, Ontological representation of social individuals, \\nOntological representation of social relationships, A ggregating and reasoning with \\nsocial network data, Representing identity, On the notion of equality, Determining \\nequality, Reasoning with instance equality, Evaluating smushing.  \\nModule IV (10 Hours)  \\nGraph Structure of the Web: Breadth First Search (BFS) Algorithm, Strongly \\nConnected Components (SCC) Algorithm, Weakly Connected Components (WCC) \\nAlgorithm, In -degree and out - degree distributions, Connected Components, Zipf’s \\nLaw, Rank Exponent R, Out -Degree Exponent O, Hop Plot Exponent H, Eigen \\nExponent E.  \\nGraph Structure of Facebook: Hyper  ANF Algorithm, Iterative Fringe Upper \\nBound (iFUB) Algorithm, Spid, Degree Distribution, Path Length, Component Size, \\nClustering Coefficient and Degeneracy, Friends -of-Friends, Degree Assortativity, \\nLogin Correlation, Effects of Age, Gender and Country of Origin.  \\nModule V (10 Hours)  \\nLink Analysis:   Search Engine – Search engine architecture, Crawling, Storage, \\nIndexing, Ranking, HITS Algorithm, Page rank  algorithm , Random walk, SALSA \\nAlgorithm, Bayesian Algorithm; Google - Google architecture, Data Structures, \\nCrawling, Searching, Web Spam Pages.  \\n \\nTextbooks.  \\n1. Social Networks and the Semantic Web, Peter Mika, Springer, 2007. (For Modules 1, 2 \\n& 3) \\n2. Practical Social Network Analysis with Python, Krishna Raj P. M., Ankith Mohan, K. \\nG. Srinivasa, Springer, 2018. (For Modules 4 & 5)  \\n \\nReferences  \\n1. Social Network Analysis, John Scott, SAGE Publications, 4th Edition (2017)  \\n2. Social Network Analysis - Interdisciplinary Approaches and Case Studies, Xiaoming \\nFu, Jar -Der Luo and Margarete Boos, CRC Press (2017)  \\n3. Handbook of Social Network Analysi s, John Scott and Peter J. Carrington, SAGE \\nPublications (2011)  \\n94 \\n 4. Social Network  Analysis  - Methods and  Applications , Stanley  Wasserman  and \\nKatherine  Faust , Cambridge University Press (2012)  \\n \\nWeb - References  \\n1. https://onlinecourses.nptel.ac.in/noc20_cs78/preview  \\n2. https://www.coursera.org/learn/social -network -analysis  \\n3. https://www.coursera.org/learn/python -social -network -analysis  \\n \\nCourse Contents and Lecture Schedule  \\n \\nNo Topic  No. of \\nLectures  \\n1 Module 1  9 Hours  \\n1.1 The Semantic Web, Limitations of the current Web  1 \\n1.2 The semantic solution  1 \\n1.3 Development of the Semantic Web  1 \\n1.4 The emergence of the Social Web  1 \\n1.5 Social Network Analysis  1 \\n1.6 Development of Social Network Analysis  1 \\n1.7 The global structure of networks  1 \\n1.8 The macro -structure of social networks  1 \\n1.9 Personal networks  1 \\n2 Module 2  8 Hours  \\n2.1 Electronic sources for network analysis, Electronic discussion \\nnetworks  1 \\n2.2 Blogs and online communities  1 \\n2.3 Web -based networks  1 \\n2.4 Knowledge Representation on the Semantic Web  1 \\n2.5 Ontologies and their role in the Semantic Web  1 \\n2.6 Ontology languages for the Semantic Web, The Resource \\nDescription Framework (RDF) and RDF Schema  1 \\n2.7 The Web Ontology Language (OWL), Comparison of Ontology \\nlanguages with the Unified Modelling Language (UML)  1 \\n2.8 Comparison to the Entity/Relationship  (E/R) model and the \\nRelational model, Comparison to the Extensible Markup \\nLanguage (XML) and XML Schema  1 \\n3 Module 3  8 Hours  \\n3.1 Modelling and aggregating social network data, Network data \\nrepresentation  1 \\n3.2 Ontological representation of social individuals  1 \\n3.3 Ontological representation of social relationships  1 \\n3.4 Aggregating and reasoning with social network data  1 \\n3.5 Representing identity  1 \\n3.6 Notion of equality, Determining equality  1 \\n95 \\n 3.7 Reasoning with instance equality  1 \\n3.8 Evaluating smushing  1 \\n4 Module 4  10 Hours  \\n4.1 Graph Structure of the Web  1 \\n4.2 Breadth First Search (BFS) Algorithm  1 \\n4.3 Strongly Connected Components (SCC) Algorithm, Weakly \\nConnected Components (WCC) Algorithm  1 \\n4.4 In-degree and out - degree distributions, Connected Components  1 \\n4.5 Zipf’s Law  1 \\n4.6 Rank Exponent R, Out -Degree Exponent O, Hop Plot Exponent \\nH, Eigen Exponent E  1 \\n4.7 Graph Structure of Facebook: HyperANF Algorithm  1 \\n4.8 Iterative Fringe Upper Bound (iFUB ) Algorithm, Spid, Degree \\nDistribution, Path Length  1 \\n4.9 Component Size, Clustering Coefficient and Degeneracy, \\nFriends -of-Friends  1 \\n4.10 Degree Assortativity, Login Correlation, Effects of Age, Gender \\nand Country of Origin  1 \\n5 Module 5  10 Hours  \\n5.1 Link Analysis: Search Engine – Search engine architecture  1 \\n5.2 Crawling, Storage, Indexing  1 \\n5.3 Ranking, HITS Algorithm  1 \\n5.4 Page rank  algorithm  1 \\n5.5 Random walk  1 \\n5.6 SALSA Algorithm  1 \\n5.7 Bayesian Algorithm  1 \\n5.8 Google - Google architecture  1 \\n5.9 Data Structures, Crawling, Searching  1 \\n5.10 Web Spam Pages  1 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n96 \\n 20MCA241  DATA SCIENCE LAB  CATEGORY  L T P CREDIT  \\nLAB    0 1 3 2 \\n \\nPreamble:  This is an introductory practical course on Data Science and student will learn how \\nto use various scientific libraries in python to implement data mining techniques and machine \\nlearning algorithms.  \\nPrerequisite:  Fundamentals of programming, python programming fundamentals, Machine \\nlearning, fundamentals of web programming,  \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Use different python packages to perform numerical \\ncalculations, statistical computations and data visualization  Level 3:  \\nApply  \\nCO 2  Use different packages and frameworks to implement \\nregression and classification algorithms.  Level 3:  \\nApply  \\nCO 3  Use different packages and frameworks to implement text \\nclassification using SVM and clustering using k -means  Level 3:  \\nApply  \\nCO 4  Implement convolutional neural network algorithm using \\nKeras framework.  Level 3:  \\nApply  \\nCO 5  Implement programs for web data mining and natural language \\nprocessing using NLTK  Level 3:  \\nApply  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 3 3 1 3 2 3  2    \\nCO 2  3 3 3 2 3 2 3  2    \\nCO 3  3 3 3 2 3 2 3  2    \\nCO 4  3 3 3 2 3 2 3  2    \\nCO 5  3 3 3 2 3 3 3  2    \\n 3/2/1: High/Medium/Low  \\n \\n \\n \\n \\n97 \\n Assessment Pattern  \\nBloom’s Category  Continuous Assessment \\nTests  End Semester \\nExamination  \\n1 2 \\nRemember (K1)     \\nUnderstand (K2)     \\nApply (K3)  50 50 50 \\nAnalyse (K4)     \\nEvaluate (K5)     \\nCreate(K6)     \\n \\nMark distribution  \\nTotal Marks  CIE ESE  ESE Duration  \\n100 50 50 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nMaximum Marks: 50  \\nAttendance  7½  \\nMaintenance of daily lab record and GitHub management  10 \\nRegular class viva  voce  7½  \\nTimely completion of day -to-day tasks  10 \\nTests/Evaluation  15 \\n \\nEnd Semester Examination Pattern:  \\nMaximum Marks: 50  \\nVerification of Daily program record and Git Repository  5 marks  \\nViva  10 marks  \\nProblem solving  \\n(Based on \\ndifficulty level, one \\nor more questions \\nmay be given)  Flowchart / Algorithm / Structured \\ndescription of problem to explain how the \\nproblem can be solved / Interface Design  15% \\n35 marks  Program correctness  50% \\nCode efficiency  15% \\nFormatted output  20% \\nCourse Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n• Review of python programming – Programs review the fundamentals of python  \\n(simple python programs ice breaker) – (at most one lab session)  \\n98 \\n • Matri x operations  (using vectorization)  and transformation using python and SVD \\nusing Python.  \\n• Programs using matplotlib / plotly / bokeh / seaborn for data visualisation.  \\n• Programs to handle data using pandas.  \\n \\nCourse Outcome 2 (CO2)   \\n• Program to implement k -NN classification using any standard dataset available in the \\npublic domain and find the accuracy of the algorithm.  \\n• Program to implement Naïve Bayes Algorithm using any standard dataset available in \\nthe public domain and find the accuracy of the algorithm  \\n• Program to implement linear and multiple regression techniques using any standard \\ndatas et available in the public domain and evaluate its performance.  \\nCourse Outcome 3(CO3):  \\n• Program to implement text classification using Support v ector machine.  \\n• Program to implement decision trees using any standard dataset available in the public \\ndomain and find the accuracy of the algorithm.  \\n• Program to implement k -means clustering technique using any standard dataset \\navailable in the public domain    \\nCourse Outcome 4 (CO4):  \\n• Programs on feedforward network to classify any standard dataset available in the \\npublic domain.  \\n• Programs on convolutional neural network to classify images from any standard dataset \\nin the public domain . \\n*[Note ] : Encourage students to refer standard neural network architectures such as LeNet5, \\nResNet, GoogLeNet etc. and use these as starting points for their models.  \\nCourse Outcome 5 (CO5):  \\nWeb Data Mining  \\n• Implement a simple web crawler  (ensure ethical conduct).  \\n• Implement a program to scrap the web page of any popular website  – suggested \\npython package is scrappy (ensure ethical conduct).  \\n \\n \\n99 \\n Natural Language Processing  \\nProblems may be designed for the following topics so that students can get hands on \\nexperience in using python for natural language processing:  \\n• Part of Speech tagging  \\n• N-gram and smoothening  \\n• Chunking  \\n \\nSyllabus  \\nReview of python programming, Matrix operations, Data Visualisation using matplotlib / \\nplotly / bokeh / seaborn, Data handling using pandas, Classification k -NN algorithm,  Naïve \\nBayes algorithm, Implementation of linear and multiple regression techniques,  Text \\nclassification using Support vector machine, Implementation of Decision Trees, Clustering \\nusing k-means algorithm, Convolutional Neural Network to classify images using Keras \\nframework, Web Crawler and Scrapping web pages, Implementation of NLP - Part of Speech \\ntagging, N -gram & smoothening and Chunking using NLTK.  \\n \\nReference Books  \\n1. Christopher M Bishop, “Pattern Learning and Machine Learning”, Springer, 2006  \\n2. E. Alpayidin, “Introduction to Machine Learning”, Prentice Hall of India (2005)  \\n3. T. Hastie, RT Ibrashiran and J. Friedman, “The Elements of Statistical Learning”, \\nSpringer 2001  \\n4. Toby Segaran, “Programming Collective Intelligence: Building Smart Web 2.0 \\nApplications”, O&#39; Reilly Media; 1 edition (16 August 2007).  \\n5. Drew Conway, John Myl es White, “Machine Learning for Hackers: Case Studies and  \\n6. Algorithms to Get You Started”, O&#39;  Reilly Media; 1 edition (13 February 2012)  \\n7. Simon Rogers, Mark Girolami, “A First course in Machine Learning”, CRC Press, \\nFirst Indian reprint, 2015.  \\n8. Tom Mitche ll, “Machine Learning”, McGraw Hill, 1997.  \\n9. Bing Liu,  ͞Web Data Mining - Exploring Hyperlinks, Contents and Usage Dat a͟, \\nSecond  edition, Springer 2011  \\n \\n \\n100 \\n Course Contents and Lab Schedule  \\n \\nSl \\nNo. Topic  No. of \\nhours  \\n1 Review of python programming, Matrix operations, Programs using \\nmatplotlib / plotly / bokeh / seaborn for data visualisation and programs to \\nhandle data using pandas.  8 \\n2 Program to implement k -NN classification using any standard dataset \\navailable in the public domain  and find the accuracy of the algorithm  2 \\n3 Program to implement Naïve Bayes Algorithm using any standard dataset \\navailable in the public domain and find the accuracy of the algorithm  2 \\n4 Program to implement linear and multiple regression techniques using any \\nstandard dataset available in the public domain  and evaluate its \\nperformance.  4 \\n5 Program to implement text classification using Support vector machine.  4 \\n6 Program to implement decision trees using any standard dataset available \\nin the public domain  and find the accuracy of the algorithm  4 \\n7 Program to implement k -means clustering technique using any standard \\ndataset available in the public domain  2 \\n8 Program on convolutional neural network to classify images from any \\nstandard dataset in the public domain  using Keras framework.  6 \\n9 Program to i mplement a simple web crawler and scrapping web pages.  6 \\n10 Implement problems on natural language processing  - Part of Speech \\ntagging , N-gram & smoothening and Chunking  using NLTK  8 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n101 \\n 20MCA2 43 MOBILE APPLICATION \\nDEVELOPMENT LAB  CATEGORY  L T P CREDIT  \\nLAB  0   1 3 2 \\n \\nPreamble:  This is a practical course on Mobile Application Development and student will \\nlearn how to program in Android Platform and develop applications using SQLite that run on \\nAndriod Operating System.  \\nPrerequisite:  Basic knowledge on programming and database co ncepts.  \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Design and develop user interfaces for mobile apps using basic \\nbuilding blocks, UI components  and application structure \\nusing  Emulator  Level 3:  \\nApply  \\nCO 2  Write simple programs and develop small applications using \\nthe concepts of UI design, layouts  and preferences  Level 3:  \\nApply  \\nCO 3  Develop applications with multiple activities using intents, \\narray adapter, exceptions and options menu.  Level 3:  \\nApply  \\nCO 4  Implement activities with dialogs , spinner, fragments and \\nnavigation drawer by  applying themes  Level 3:  \\nApply  \\nCO 5  Develop mobile applications using SQLite . Level 3:  \\nApply  \\n \\nMapping of course outcomes with program outcomes  \\n PO 1  PO 2  PO 3  PO 4  PO 5  PO 6  PO 7  PO 8  PO 9  PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 3 3 1 3 2 3  2    \\nCO 2  3 3 3 2 3 2 3  2    \\nCO 3  3 3 3 2 3 2 3  2    \\nCO 4  3 3 3 2 3 2 3  2    \\nCO 5  3 3 3 2 3 3 3  2    \\n 3/2/1: High/Medium/Low  \\n \\n \\n \\n \\n \\n \\n102 \\n Assessment Pattern  \\nBloom’s Category  Continuous Assessment \\nTests  End Semester Examination  \\n1 2 \\nRemember(K1)     \\nUnderstand(K2)     \\nApply(K3)  50 50 50 \\nAnalyse(K4)     \\nEvaluate(K5)     \\nCreate(K6)     \\n \\nMark distribution  \\nTotal Marks  CIE ESE  ESE Duration  \\n100 50 50 3 hours  \\n \\nContinuous Internal Evaluation Pattern:  \\nMaximum Marks: 50  \\nAttendance  7½  \\nMaintenance of daily lab record and GitHub \\nmanagement  10 \\nRegular class viva  voce  7½  \\nTimely completion of day -to-day tasks  10 \\nTests/Evaluation  15 \\n \\nEnd Semester Examination Pattern : \\nMaximum Marks: 50  \\nVerification of Daily program record and Git Repository  5 marks  \\nViva  10 marks  \\nProblem solving  \\n(Based on \\ndifficulty level, one \\nor more questions \\nmay be given)  Flowchart / Algorithm / Structured \\ndescription of problem to explain how the \\nproblem can be solved / Interface Design  15% \\n35 marks  Program correctness  50% \\nCode efficiency  15% \\nFormatted output  20% \\n \\n \\n \\n \\n \\n103 \\n Course Level Assessment Questions  \\n Course Outcome 1 (CO1):  \\n1. Design a Login Form with username and password using LinearLayout and toast valid \\ncredentials  \\n2. Write a program that demonstrates Activity Lifecycle.  \\n3. Implementing basic arithmetic operations of a simple calculator  \\n4. Implement validati ons on various UI controls  \\nCourse Outcome 2 (CO2)   \\n1. Design a registration activity and store registration details in local memory of phone \\nusing Intents and SharedPreferences  \\n2. Design a simple Calculator using GridLayout and Cascaded LinearLayout   \\n3. Create a F acebook page using RelativeLayout; set properties using .xml file  \\n4. Develop an application that toggles image using FrameLayout  \\nCourse Outcome 3(CO3):  \\n1. Implement Adapters and perform exception handling  \\n2. Implement Intent to navigate between multiple activities  \\n3. Develop application that works with explicit intents  \\n4. Implement Options Menu to navigate to activities  \\n5. Develop an application that uses ArrayAdapter with ListView.  \\nCourse Outcome 4 (CO4):  \\n1. Develop an application that use GridView with images and display Ale rt box on \\nselection  \\n2. Develop an application that implements Spinner component and perform event \\nhandling  \\n3. Apply themes via code and manifest file  \\n4. Develop application using Fragments  \\n5. Implement Navigation drawer  \\nCourse Outcome 5 (CO5):  \\n1. Create database using SQLite and perform INSERT and SELECT  \\n2. Perform UPDATE and DELETE on SQLite database  \\n3. Develop an application as a micro project which uses SQLite database  as an \\nassignment  \\n \\n \\n \\n \\n \\n104 \\n Syllabus  \\nFundamentals:  Basic Building blocks – Activities, Services, Broadcast Receivers and Content \\nproviders, UI Components – Views and notifications Components for communication -Intents \\nand Intent Filters  \\nApplication Structure:  AndroidManifest.xml, user -permission – sdk, R esources and R.java,  \\nAssets, Layouts and Drawable Resources, Activities and Activity lifecycle.  \\nEmulator -Android Virtual Device:  Launching emulator, Editing emulator settings, Emulator \\nshortcuts, Logcat usage, Introduction to DDMS  \\nBasic UI design:  Form w idgets, Text Fields, Validation of EditText, Layouts, [dip, dp, sip, sp] \\nversus px  \\nPreferences:  Shared Preferences, Preferences from xml  \\nMenu:  Option menu, Context menu, menu from xml, menu via code  \\nIntents:  Explicit Intents, Implicit intents  \\nUI design:  Time and Date, Images and media, Android Adapter and ListView, Composite, \\nAlert Dialogs and Toast, Popup, Fragments, Navigation drawer   \\nTabs, Tab Activity Styles & Themes:  styles.xml, drawable resources for shapes, gradients \\n(selectors), style attribute in layout file, Applying themes via code and manifest file  \\nContent Providers:  SQLite Programming, SQLite Open Helper, SQLite Database, Cursor, \\nReading and updating Contacts, Reading bookmarks  \\n \\nReference Books   \\n1. Joseph Annuzzi Jr, Lauren Darcey, Shane Condor, “Advanced Android Application \\n Development, Developers Library”, Pearson Education, 4th Edition (2015)  \\n2. Lauren Darcey, Shane Condor, “Android, Wireless Application Development”, \\nPearson Education, 3rd Edition.  \\n3. Paul Deitel, Harvey Deitel, Alexander Wald, “Android 6 for programmers, An \\nAppDriven Approach”, Pearson Education  \\n4. Rap Payne, “Beginning App Development with Flutter: Create Cross -Platform \\nMobile Apps”, Apress (2019)  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n105 \\n Course Contents and Lecture Schedule  \\nSl No  Topic  No. of \\nhours  \\n1 Fundamentals – Basic building blocks  3 \\n2 Application structure, layout and resources  3 \\n3 Android Virtual Device, Activity Lifecycle  3 \\n4 Basic UI Design and EditText  Validation  4 \\n5 Shared Preferences, RelativeLayout, FrameLayout, GridLayout \\nand Preferences from xml  9 \\n6 ArrayAdapter, ListView and Exception handling  3 \\n7 Various Menu options  3 \\n8 Explicit and Implicit Intents  3 \\n9 Images and media, Dialogs, Spinner component, Popups, \\nFragments, Navigation drawer  6 \\n10 Applying themes and styles .xml  3 \\n11 SQLite Programming  6 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n106 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA245  MINI PROJECT  PROJECT  - - 4 2 \\n \\nPreamble: This project work aims to enable the students to apply the software engineering \\nprinciples on a real software project, t o make the students familiar with the stages of a \\ndeployment pipeline and to d evelop a software product using the latest software development  \\nmethodology.  \\n \\nPrerequisite: Knowledge in software engineering principles and programming skills.   \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Identify a real-life project which is useful to society / industry  Level 2:  \\nUnderstand  \\nCO 2  Interact with people to identify the project requirements  Level 3:  \\nApply  \\nCO 3  Apply suitable development methodology for the development \\nof the product / project  Level 3:  \\nApply  \\nCO 4  Analyse and design a  software product  / project  Level 4:  \\nAnalyse  \\nCO 5  Test the modules at various stages of project development  Level 5:  \\nEvaluate  \\nCO 6 Build and integrate different software modules  Level 6:  \\nCreate  \\nCO 7 Document and deploy the product / project  Level 3:  \\nApply  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 3 3 3 1 2 3 3 3 3 3 3 \\nCO 2  2 3 2 3 2 3 2 1 3 2 3  \\nCO 3  3 3 3 3 3 1 3 3 1  2  \\nCO 4  3 3 3 3 3  3 3 1 1 2  \\nCO 5  3 3 3 3 3  2 3   1  \\nCO 6 3 3 3 3 3 2 3 3  2 3 3 \\nCO 7 1 1 3 3 3 2 3 3 2 1 2  \\n 3/2/1: High/Medium/Low  \\n \\n \\n107 \\n Mark distribution  \\nTotal Marks  CIE ESE  \\n100 100 - \\n \\nAssessment Criteria  \\n \\nClass participation and attendance  10% \\nEvaluation  50% \\nClass work  40% \\n \\nMarks Division  \\n \\nContinuous evaluation by Supervisor, Scrum \\nMaster and Project Guide  50 Marks  \\nInterim evaluation by the Project Assessment \\nBoard  25 Marks  \\nFinal evaluation by the Project Assessment \\nBoard  25 Marks  \\nTotal  100 Marks  \\n \\nGuidelines : \\n \\n● Students shall identify Real -Life Pro jects which are relevant and useful to the society \\nor industry.  \\n \\n● The project  shall be an individual project and  must be done in -house. The student has \\nto spend  time in the lab for the project work.  \\n \\n● Attendance as per MCA regulations is applicable for submitting the project for final \\nevaluation.  \\n \\n● Students shall submit project synopsis and get prior approval from the Project (Faculty) \\nSupervisor before the project work  begins .   \\n \\n● If there is a customer for the project then he /she will be the Product Owner (External \\nGuide) and a faculty from  the department will be the Internal Guide. If there is no such \\ncustomer then the Internal Guide himself /herself  shall act as the Product Owner.  \\n \\n● A faculty / technical staff shall act as the Scrum Master to continuously monitor the \\nproject development. Periodic  meetings, of less than 15 minutes, at the convenience  of \\n108 \\n the Scrum Master are to be highly encouraged. Ensure such meetings occur once in \\nthree days.  \\n \\n● Set a sprint as two weeks, ensure biweekly reviews. A review shall not exceed 30 \\nminutes. A demo to the Product Owner (Project Guide) is mandatory in every  review.  \\n \\n● The student shall maintain a Scrum Book (Rough Record) which has to be divided into \\n4 parts – (i) Product Backlog (ii) Database & UI Design (iii) Testing & Validation and \\n(iv) Details of Versions. Make dated entries in the corresponding part at regular \\nintervals. The corrections and comments from Product Owner and Scrum Master should \\nbe clearly indicated with the Date.  \\n \\n● Test Driven Development methodology may be practiced for the project development. \\nBugZilla, BackLog or any  such tool may be used for Bug Tracking.  \\n \\n● Git shall be used for Version Control and Git commit history  may be verified as part of \\nproject evaluation      . \\n \\n● LaTeX  or an equivalent tool shall be used for preparing Presentations and Project \\nReport.  \\n \\n● Interim evaluations of project ’s progress shall be conducted as part of Internal \\nAssessment. Project Evaluation Board may consist of Project Supervisor, Product \\nOwner, Scrum Master and one other Faculty Member from  the department. Scrum \\nreviews shall not be sacrificed for such presentations.  \\n \\n● At the end of the semester entire project development activities shall be evaluated \\ninternally by the Project Evaluation Board .  \\n \\n \\nWeek  Schedule  \\n1 Familiarisation with build tools  (editor/IDE, compiler such as gcc with \\ncommonly used options/switches, debugger like dgb).  \\nFamiliarisation with an IDE (Eclipse, NetBeans…), that support s build tools and \\ncommon version control operations using Git      . \\nFamiliarisation with Docker  \\nSelection of Topic, Formation of Development Team, Feasibility analysis.  \\n2 Topic Approval, Meeting of Development Team including Scrum Master with \\nProduct Owner.  \\nInformal, preliminary discussions of requirements.  \\nCreating user  stories in the rough record.  \\n109 \\n Commencement of the Project.  \\n3 Identifying modules, Initial Design of Database & UI.  \\nCreating a Docker container for the environment  \\nCreating an empty git repository by Scrum Master / one member of the \\nDevelopment team and setting permission to other members.  \\nPushing the first version of the Project along with a Readme file containing \\ncontact details of team members.  \\nCreating pull requ ests for sample  update of Readme by each member and \\nmerging the pull requests of one by another.  \\n4-5 Setting up systems for development, testing and production.  \\nDesign of the basic model of a simple deployment pipeline  \\nCreating a suitable folder structure (Maven’s folder structure is desirable). \\nCreating Unit tests using an XUnit  framework , Writing the build and code \\nanalysis script,  Writing acceptance test s cripts and test cases, Setting up a \\nContinuous Integration System like Jenkins. Automating acceptance tests with \\nSelenium , Karate  or an equivalent tool, writing a simple deployment script that \\nuses scp/rsync or Ansible for copying the Dockerfile and running Docker with \\nssh.  \\nFirst Scrum Review.  \\n(Here onwards, the Scrum reviews are conducted on every other week)  \\n7 Project Presentation - Interim  \\nEvaluation to be based on Git History  \\n14 Submission of Project Report, with Scrum Book  \\nProject Presentation – Final  \\nEvaluation to be based on Git History, Scrum Book, Project Report and \\nPresentation  \\n \\n \\n \\n \\nReferences  \\n1. Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment \\nAutomation (Addison -Wesley Signature Series (Fowler)) 1st Edition  \\n2. Alistair Cockburn, Agile Software Development: The Cooperative Game, Addison \\nWesley, 2nd Edition (2006).  \\n3. Andrew Hunt, David Thomas, The Pragmatic Programmer: From Journeyman to \\nMaster, Pearson India, 1st Edition (2008).  \\n110 \\n 4. Ken Schwaber, Mike Beedle, Agile Software Development with Scrum, Pearson \\n(2008).  \\n5. Lisa Crispin, Janet Gregory, Agile Testing: A Practical Guide for Testers and Agile \\nTeams, Addison Wesley Professional, 1st Edition (2008).  \\n6. Mike Cohn, User Stories Applied: For Agile Softw are Development, Addison Wesley, \\n1st Edition, (2004).  \\n7. Pressman, R.S., Software Engineering: A Practitioner\\'s Approach, McGraw Hill SE, \\n7th Edition, (2010).  \\n8. Robert C. Martin, Agile Software Development, Principles, Patterns and Practices, \\nPrentice Hall Impr int, Pearson Education, 2nd Edition (2002). \\\\ \\n9. Rod Stephens, \\U0010035eBeginning Software Engineering, Wrox Series, Wiley India Pvt Ltd \\n(2015).  \\n10. RyPress Ry\\'s Git Tutorial (Free e -book)  \\n \\nWeb Reference  \\n \\n1. Introduction to DevOps (https://www.edx.org/course/introduction -devops -microsoft -\\ndev212x)  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n111 \\n CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCANC3   Domain Expertise Workshops  Non-Credit \\nCourse  - - 1 Nil \\n \\nPreamble: This course intends to give insight into  various application domains and technology \\ndomains in IT industry.   \\n \\nPrerequisite: Nil  \\n \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Associate real -life problems with IT solutions  Level 2:  \\nUnderstand  \\nCO 2  Describe latest developments in IT field  Level 2:  \\nUnderstand  \\nCO 3  Interact with technical experts  Level 3:  \\nApply  \\nCO 4  Prepare technical documents  Level 3:  \\nApply  \\nCO 5  Present a topic before an audience  Level 2:  \\nUnderstand  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  1 2    3 3  3 3 3 3 \\nCO 2  2 2    3 3  3 3 3 3 \\nCO 3   2    3 3  3   3 \\nCO 4       3   3   3 \\nCO 5       3   3   3 \\n 3/2/1: High/Medium/Low  \\n \\n \\n \\n112 \\n Mark distribution  \\nTotal Marks  CIE ESE  \\n- - - \\n \\nGuidelines : \\n• As part of this course following activities shall be done:  \\n➢ Expert talks shall be arranged to explain about  various  Application domains like \\nRetail, Finance, Healthcare, Automotive , Manufacturing … and IT domains  like \\nIoT, AI, Bigdata, Full Stack Development, Robotic Process Automation…  \\n➢ Instruct students to research and submit reports about any of these domains . \\n➢ Instruct students to study about  these domains  and take seminars…   \\n \\n• One hour in every week or two hours in alternate weeks shall be used for this course  \\n \\n• Staff -in-charge shall maintain a file with the records, documents and reports as \\nhardcopies or e -copies of all the activities done on this course.  \\n \\n• These documents shall be submitted for verification during the academic audit.  \\n \\n \\n \\n \\n \\n \\n \\n113 \\n \\nSEMESTER -4  \\n \\n \\n \\n \\n \\n \\n \\n114CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA24 2 COMPREHENSIVE VIVA  VIVA  - - - 6 \\n \\nPreamble: Comprehensive Viva  intends to assess the knowledge gained by a student  in the \\ncore courses of this programme  and to make the student aware of his/her  knowledge level and \\nwhere he/she  stand s after completing this programme.  This course will help the student in \\npreparing for comprehensive examinations and improve the confidence in answering questions \\nin objective mode.  \\n \\nPrerequisite: Thorough knowledge i n all the courses he/she learned during this programme.   \\n \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Articulate the concepts in the core courses learned through this \\nprogramme.  Level 2:  \\nUnderstand  \\nCO 2  Attend technical interviews with confidence.  Level 2:  \\nUnderstand  \\nCO 3  Interpret questions and answer them with  clarity.  Level 2:  \\nUnderstand  \\nCO 4  Make use of the concepts learned through this programme in \\nfuture.  Level 3:  \\nApply  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  3 3  2 2  2  3    \\nCO 2  3 3 1 2 3 2 3  3    \\nCO 3  1 2    2 2  3    \\nCO 4  3 2 3 2 2 3 3  2    \\n \\nMark distribution  \\nTotal Marks  CIE ESE  \\n100 - 100 \\n \\n \\n \\n115Guidelines : \\n \\n• Comprehensive viva shall be conducted within the first 20 days of the fourth semester.  \\n \\n• Viva shall be conducted by a panel of examiners consisting of:  \\n1. Head of the department  \\n2. A senior faculty in the department  \\n3. External examiner appointed by the university  \\n \\n• Viva shall be conducted for each student for a minimum of 20 minutes  \\n \\n• Knowledge level of the student shall be assessed on the following topics.  \\n \\n▪ 20MCA105 - Advanced Data Structures  \\n▪ 20MCA107 - Advanced Software Engineering  \\n▪ 20MCA102 - Advanced Database Management Systems  \\n▪ 20MCA201 - Data Science & Machine Learning  \\n▪ 20MCA203 - Design & Analysis of Algorithms  \\n▪ Code snippets in Java or Python to solve simple problems . \\n▪ Technologies used in the project  work   \\n▪ Recent developments in the field of computer science.               \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n116CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA244  SEMINAR  SEMINAR  - - 2 2 \\n \\nPreamble: This course intends to e nable the students to gain knowledge in any of the \\ntechnically relevant current topics on Computer Science or Information Technology, and to \\nacquire confidence in presenting the topic and preparing a report.  \\n \\nPrerequisite: Nil  \\n \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Annotate the ideas presented in technical papers  Level 2:  \\nUnderstand  \\nCO 2  Comprehend a concept by referring different technical \\ndocuments  Level 2:  \\nUnderstand  \\nCO 3  Prepare technical documents  Level 3:  \\nApply  \\nCO 4  Present a topic before an audience  Level 3:  \\nApply  \\nCO 5  Interact with the audience  Level 2:  \\nUnderstand  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 3 1 3 2  3  3 2  2 \\nCO 2  2 3 1 3 2  3  3 2  2 \\nCO 3  2  1 2 3 2 3  3 2  2 \\nCO 4  2 2   3 3   3  2  \\nCO 5  2 2   3 3   3  2  \\n \\n \\n \\n117Mark distribution  \\nTotal Marks  CIE ESE  \\n50 50 - \\n \\nAssessment Criteria  \\n \\nScope and relevance of topic  20% \\nQuality of presentation slides  10% \\nPresentation skills  30% \\nKnowledge in the topic  20% \\nReport  20% \\n \\nMarks Division  \\n \\nEvaluation by Faculty Guide  20 Marks  \\nEvaluation by the Faculty Committee  30 Marks  \\nTotal  50 Marks  \\n \\nGuidelines : \\n• Students shall conduct detailed study on a technically relevant current topic in \\nComputer Science / Information Technology under the supervision of a Faculty Guide \\nand present it as a seminar at the end of the study.  \\n• The study may be conducted on  \\n▪ articles published in reputed journals/conference proceedings  \\n▪ recent development in Computer Science / Information Technology  \\n▪ recent research and development activity in a research lab  \\n▪ latest software tool or fra mework  \\n \\n• Students shall submit an abstract on identified topic and get prior approval from the \\nFaculty Guide before the study  begins . \\n• The student shall submit a seminar report, based on the study and their findings. The \\nreport shall not be a reproduction of original paper or manual.  \\n• The study and its findings shall be presented in the class taking a duration of 15 -20 \\nminutes.  \\n● LaTeX  or an equivalent tool shall be used for preparing Presentations and Seminar \\nReport.  \\n● Students shall be encouraged to publish their study in journals and due credit shall be \\ngiven to such students.  \\n• A committee of three senior faculty members shal l constituted by the head of the \\ndepartment and the seminar presentation shall be evaluated by that committee.  \\n \\n118CODE  COURSE NAME  CATEGORY  L T P CREDIT  \\n20MCA246  MAIN PROJECT  PROJECT  - - 27 12 \\n \\nPreamble: This project work aims to enable the students to apply the software engineering \\nprinciples on a real software project, t o make the students familiar with the stages of a \\ndeployment pipeline and to d evelop a software product using the latest software development  \\nmethodology.  \\n \\nPrerequisite: Knowledge in software engineering principles and programming skills.   \\nCourse Outcomes:  After the completion of the course the student will be able to  \\nCO No.  Course Outcome (CO)  Bloom\\'s \\nCategory Level  \\nCO 1  Identify a real-life project which is useful to society / industry  Level 2:  \\nUnderstand  \\nCO 2  Interact with people to identify the project requirements  Level 3:  \\nApply  \\nCO 3  Apply suitable development methodology for the development \\nof the product / project  Level 3:  \\nApply  \\nCO 4  Analyse and design a  software product  / project  Level 4:  \\nAnalyse  \\nCO 5  Test the modules at various stages of project development  Level 5:  \\nEvaluate  \\nCO 6 Build and integrate different software modules  Level 6:  \\nCreate  \\nCO 7 Document and deploy the product / project  Level 3:  \\nApply  \\n \\nMapping of course outcomes with program outcomes  \\n PO \\n1 PO \\n2 PO \\n3 PO \\n4 PO \\n5 PO \\n6 PO \\n7 PO \\n8 PO \\n9 PO \\n10 PO \\n11 PO \\n12 \\nCO 1  2 3 3 3 1 2 3 3 3 3 3 3 \\nCO 2  2 3 2 3 2 3 2 1 3 2 3  \\nCO 3  3 3 3 3 3 1 3 3 1  2  \\nCO 4  3 3 3 3 3  3 3 1 1 2  \\nCO 5  3 3 3 3 3  2 3   1  \\nCO 6 3 3 3 3 3  3 3  2 3 3 \\nCO 7 1 1 3 3 3 2 3 3 2 1 2  \\n \\n119Mark distribution  \\nTotal Marks  CIE ESE  \\n100 70 30 \\n \\nMarks Division  \\n \\nContinuous evaluation by Supervisor, \\nGuide(s) and Scrum Master  30 Marks (Internal)  \\nEvaluation by the Project Assessment Board  40 Marks (Internal)  \\nEvaluation by the External expert  30 Marks (External)  \\nTotal  100 Marks  \\n \\nGuidelines:  \\n \\n● Students shall identify Real -Life Pro jects which are relevant and useful to the society \\nor industry.  \\n \\n● The project  shall be an individual project and  must be done in -house. The student has \\nto spend  time in the lab for the project work. Attendance as per MCA regulations is \\napplicable for submitting the project for final evaluation.  \\n \\n● However, in exceptional cases students shall be given permission to work on the project \\noutside the campus and a t the industry premises if the organization offering the project \\nbelongs to anyone of the following categories.  \\n▪ CMM Level 5 Certified Company  \\n▪ Publicly listed company in India  \\n▪ National Research Institute  \\n▪ Central / State Government Department  \\n▪ Project funded by the Central / State Government Agency  \\n \\n● In such cases, the student is required to produce a letter from the organisation before \\nstarting the project and a committee constituted by the head of the department shall \\nmake the decision on permission. I ndustries and training institutes that offer project  \\nwork for a fee  shall not be permitted.  \\n \\n● Students shall submit project synopsis and get prior approval from the Project (Faculty) \\nSupervisor before the project work  begins .   \\n \\n120● If there is a custo mer for the project then he /she will be the Product Owner (External \\nGuide) and a faculty from  the department will be the Internal Guide. If there is no such \\ncustomer then the Internal Guide himself /herself  shall act as the Product Owner.  \\n \\n● A faculty  / technical staff shall act as the Scrum Master to continuously monitor the \\nproject development. Periodic  meetings, of less than 15 minutes, at the convenience of \\nthe Scrum Master are to be highly encouraged. Ensure such meetings occur once in \\nthree  days.  \\n \\n● The student shall maintain a Scrum Book (Rough Record) which has to be divided into \\n4 parts – (i) Product Backlog (ii) Database & UI Design (iii) Testing & Validation and \\n(iv) Details of Versions. Make dated entries in the corresponding part at re gular \\nintervals. The corrections and comments from Product Owner and Scrum Master should \\nbe clearly indicated with the Date.  \\n \\n● Test Driven Development methodology may be practiced for the project development. \\nBugZilla, BackLog or any such tool may be used f or Bug Tracking.  \\n \\n● Git shall be used for Version Control and Git commit history  may be verified as part of \\nproject evaluation      . \\n \\n● LaTeX  or an equivalent tool shall be used for preparing Presentations and Project \\nReport.  \\n \\n● Students shall be encouraged to publish their work in journals and due credit shall be \\ngiven to such students.  \\n \\n● For the externally done projects, periodic confidential progress report and attendance \\nstatement shall be collected from the External Guide and be reviewed by the  Project \\nSupervisor.  \\n \\n● Set a sprint as two weeks, ensure biweekly reviews. A review shall not exceed 30 \\nminutes. A demo to the Product Owner (Project Guide) is mandatory in every  review.  \\n \\n● Interim evaluations of the project ’s progress shall be conducted  by a Project \\nAssessment  Board  as part of internal assessment.  Two such evaluations are desirable. \\nScrum reviews sh all not be sacrificed for such presentations.  \\n \\n● The Project Assessment Board  shall be constituted by the Head of the Department with \\nthe following five members.  \\nChairman:   \\n1. Head of the Department  \\nMembers:   \\n2. Project supervisor/s of the student  \\n1213. One faculty member from the Department  \\n4. One faculty member from a sister Department  \\n5. An external expert, either from an academic/research institute or \\nIndustry . (For the externally done projects, the external guide shall be \\ninvited as external expert.)  \\n \\n● At the end of the semester , two evaluations shall be there on the entire project \\ndevelopment activities. First an internal evaluation by the Project Assessment Board  \\nand second an external evaluation by an External Examiner.  \\n \\n● An External Examiner either from an academic institute or  industry shall  be appointed \\nby the University for the External Evaluation.  \\n \\nWeek  Schedule  \\n(May be scheduled inline with the KTU academic calendar)  \\n1 Selection of Topic , Submission of project synopsis and getting approval  \\nMeeting of Development Team including Scrum Master with Product Owner \\n(Project Guide)  \\n2 Commencement of the Project.  \\n4 First Sprint release and Scrum Review by the Product Owner (Project Guide)  \\n6 Second Sprint release and Scrum Review by the Project Guide  \\nFirst interim evaluation by the Project Assessment Board  \\n8 Third Sprint release and Scrum Review by the Project Guide  \\n10 Fourth Sprint release and Scrum Review by the Project Guide  \\n11 Second interim evaluation by the Project Assessment Board  \\n12 Fifth Sprint release and Scrum Review by the Project Guide  \\n13 Submission of project report, with Scrum Book  \\nFinal  project presentation  \\nEvaluation by the Project Assessment Board  \\n14  Final eva luation by the External Examiner.  \\n \\n122'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VP6ap_PSRt7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9GLXwH1SVOe",
        "outputId": "ba0af64f-6868-4b85-f045-34988510a3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r68CZl2Lvvr",
        "outputId": "0bda3e8e-aefe-40dd-eba0-e87a5591809d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.16-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.17 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.18)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.35.13)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (0.1.85)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.17->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.17->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.17->langchain-openai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.17->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.17->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Installing collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "wqy4vJhrSXUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "ExayA0J_MFL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bJpE1qlSlNb",
        "outputId": "2eebaa45-b187-4f57-da0e-625940ae5754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.vectorstores.faiss.FAISS at 0x7f7abd1445b0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "Po-ip1fPSonv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "iYl2PzKSSqg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Vision for Amrit Kaal\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "GQafhpOz4IsV",
        "outputId": "b05ede70-3d69-40ee-a4af-ba0bd14659b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Our vision for the Amrit Kaal includes technology-driven and knowledge-based economy with strong public finances, and a robust financial sector. To achieve this, Jan Bhagidari through Sabka Saath Sabka Prayas is essential. The economic agenda for achieving this vision focuses on three things: first, facilitating ample opportunities for citizens, especially the youth, to fulfil their aspirations; second, providing strong impetus to growth and job creation; and third, strengthening macro-economic stability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How much the agriculture target will be increased to and what the focus will be\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7sjc1Xh2SsTs",
        "outputId": "2c3ef36a-4732-4f54-d704-eda850c292a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The agriculture credit target will be increased to ` 20 lakh crore with focus on animal husbandry, dairy and fisheries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}